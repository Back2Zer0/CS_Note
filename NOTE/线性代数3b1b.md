**前言：**

> 一个很久之前截的B站评论:
>
> @？
>
> 很多人学不懂线代(高代)的原因是，线性代数这门课本质上来说是 具体-抽象-再具体 的回旋过程首先从线性方程组开始讲起，讨论矩阵，方阵的性质然后再由矩阵上升到线性空间上的线性映射线性变换，在研究线性映射和线性变换时又会通过和数域K的联系再具体化到矩阵所以入门学习矩阵的时候，很容易迷失在处处严谨的证明细枝未节里，从而没法把握核心的思路与研究目的;
>
> 而矩阵没学好，后面的线性空间更难理解，因为线性空间研究往往是再具体化到矩阵上来进行的
>
> 所以我建议朋友们学习高代(线代)的时候，在初期就把这门课当作一门外语来学在矩阵部分对每一条定理每一个命题都把他当作单词吃透当你带着扎实的矩阵功底去接触线性空间的时候，马上如拨云见日般。你也会很快感受到代数学具体抽象再具体的魅力。
>
> @吸猫群众QAQ:
>
> 数无形时少直觉，形少数时难入微，数形结合百般好。越学习越惊叹于古人的智慧，三百多年的科学史，孕育出了多少绝妙的思想。想到了年纪轻轻却在数学体系不完善的年代创造出微积分，用古老的几何证明法得出若干现在仍被奉为真理的几大定律，却在晚年困惑于神与世界本质的思辨中的牛顿;还有靠现有的数学工具以及其绝妙的思维创造出广义相对论，以一人之力颠覆了人类对空间和时间这两个玄妙概念的理解，在晚年试图创造出可以解释世界的终极定理却终究敌不过时间的爱因斯坦;还有那些出于对一个奇怪的物理现象的探究，却揭示出一个光怪陆离，极度违背常识，连创始人都无法理解，却又无数次被证实是正确的理论的一众量子力学的奠基人们。人类文明的智慧闪光真的令人叹为观止，或许无尽岁月之后，有足够辨析能力的另一个文明看到或许已经灭绝的人类残存的信息，也会惊叹或惋惜于这个文明曾经的智慧吧。
>
> @卧三：
>
> 我所理解的秩，是在某个维度下所固定的条件个数，比方说，三维空间我们有x，z，y三个线性无关的条件，三个条件对应着秩为3，即秩为3时我们可以确定空间的一个点。对应此时我们只有唯一解，即为R(A) = n ，假如三维空间中，我们秩为2 即为R(A) < n ,我们们只有两个量假设x，y是固定的或者说已知的，此时我们将会有一个平面的点z不确定的点。所以我们将有一个自由变量，即为z方向上的自由变量，我们用一个向量表示这些有的点，则为我们的通解。所以维度 = 秩（确定的量）+ 基础解系个数(不确定的量)

目录：

[TOC]




## 0.线性代数的“线性”

- “线性”的严格定义如下若一个变换工满足两条性质：**齐次性和可加性**。



1. 可加性:
   $$
   L(\vec{v} +\vec{w}) = L(\vec{v})+L(\vec{w})
   $$

2. 齐次性（成比例）
   $$
   L(c\vec{v}) = cL(\vec{v})
   $$

则称L是线性的。

> 从几何角度看，
>
> 将`i向量`固定，移动 `j向量`，将形成一条直线；
>
> 将`j向量`固定，移动 `i向量`，将形成另一条直线；
>
> 每次移动向量，都是对应基向量的倍数（成比例），而基向量之间可以相加形成新的向量。
>
> 可加、可乘（成比例）
>
> 如果固定其中一个标量，让另一个标量自由变化，所产生的向量的终点会描出一条直线。
>
> **基向量线性组合的向量的集合，被称为给定向量张成的空间(span)**

- 线性的词源:出于线性方程组。线性方程组在二维情况下，有形如ax+by=c的子方程

  所谓线性关系，简单地讲就是比例关系，即两个变量按一定的比例增加或减少。

  **这种关系若用图形来表示，就是一条直线，故称线性关系。**这种关系若用方程来表示，就称为线性方程

- 线性代数 这门学科之所以包含“线性”二字，是因为它主要研究的是**向量空间（或称为线性空间）、线性变换以及线性方程组等概念。**这些概念都涉及到线性关系：



## 1.线性相关

可以理解为线性相关意味着停留在这个维度不增维。当一组向量成线性相关时，其中至少有一个向量能由其它向量线性表示

## 2.基向量

空间的一组基的严格定义是这样的:张成该空间的一个线性无关向量的集合。

（向量空间的一组基是张成该空间的一个线性无关向量集）

## 3.线性变换（劲爆）

严格意义上说，线性变换是将向量作为输入和输出的一类函数。

线性变换是操纵空间的一种手段。

它具备两个性质：

**①保持网格线平行且等距分布，②保持原点不动。**

这两点性质保证了：只要记录下i帽和j帽变换后的位置，你就能计算出一个坐标为(x,y)的向量变换后的坐标。

令人高兴的是，这种变换只需要几个数字就能描述清楚，这些数字就是变换后基向量的坐标。**线性变换由它对空间的基向量的作用完全决定。**

> 习惯上，我们将变换后i帽和j帽的坐标作为一个矩阵的列，并且将两列分别与x和y相乘后加和的结果定义为矩阵向量乘积

这样，矩阵代表一个特定的线性变换。而矩阵与向量相乘，就是将线性变换作用于那个向量。

因此**可以将线性变换看作对空间的挤压伸展。**这是因为其他任意向量都能表示为基向量的线性组合。

---

线性相关时，如果变换后的i帽和变换后的j帽是线性相关的，意味着其中一个向量是另一个的倍数，那么这个线性变换将整个二维空间挤压到它们所在一条直线上。也就是这两个线性相关向量所张成的一维空间。

以这些坐标为列所构成的矩阵为我们提供了一种描述线性变换的语言，而矩阵向量乘法就是计算线性变换作用于给定向量的一种途径。

这里重要的是。**每当你看到一个矩阵时，你都可以把它解读为对空间的一种特定变换。**

矩阵相乘：两个变化相互作用，即复合变化。这里可以联想复合函数。



**这样也可以解释矩阵的两个性质：**

1. 不具备交换律

   AB ≠ BA

   也就是为什么矩阵乘法有严格顺序要求。例如 先旋转、再剪切 和 先剪切、再旋转是不同的。

2. 具备结合律

   A(BC) 和 (AB)C 三个线性变化的相对顺序不变，

>线性代数中的“剪切”通常指的是剪切矩阵（shear matrix）或剪切变换（shear transformation），这是一种线性变换，它将一个图形沿着某一轴或平面进行倾斜，而不改变图形的大小或形状

这是通过矩阵在几何上的线性变换，证明矩阵乘法具有结合性的一个实实在在的证明。我真的鼓励你在这种想法上多做尝试，想象两个不同的变换，思考他们依次作用后会发生什么，最后用数值方法计算出矩阵乘积。

4.观察基向量的运算和线性变换，**“缩放再相加”**的过程在变换前后均适用。

对于一个矩阵，用一个元素均为x、y、z这种未知数的向量相乘，得到的新矩阵就是新的基向量。

**线性变换矩阵 \* 输入向量 = 输出向量。**

（线性变换和函数的功能是类似的，输入-处理-输出。但变换一词更强调变换本身的过程，也就是几何而非数值上的变化。）



## 4.行列式

不同矩阵代表的线性变换中，有的将空间向外拉伸，有的将空间向内挤压。理解这些线性变换的关键一点，是测量变换对空间拉伸或挤压的程度。也就是**测量一个给定区域面积增大或减小的比例**。

这个特殊的缩放比例，即**线性变换改变面积的比例 , 被称为这个变换的行列式**



**注意：**

1. 无论一个方格如何变化，对其他大小的方格来说，都会有相同变化。

   > 这是由“网格线保持平行且等距分布”这一事实推断得出的

2. 对于不是方格的形状（存在曲线的），可以用很多小方格近似。

   > 对所有小方格都进行等比例缩放，缩放的越小，精度越高。

3. 如果一个二维线性变换的行列式为0，说明它将整个平面压缩到一条线，甚至是一个点上。

   > 表现在几何上就是面积为0，没有面积。 	

   这意味着一个矩阵的行列式如果为0，这个矩阵所代表的变换就能将空间压缩到更小的维度上（降维）。



**二维空间的行列式负值**：

**定向**

> 行列式有负值。缩放一个负数是什么意思?
>
> 这会涉及到一个叫“定向”的概念。

1. 如里你将二维空间想象为一张纸，这个变换像是将纸翻转到了另一面。

2. 根据`i帽`和`j帽`来考虑。

   初始状态下，显然` j-hat` 在` i-hat` 左边，形成一个直角。

   **如果在变换之后，j帽处于i帽的右边，那么空间定向就发生了改变**

   > `j 帽`就是y轴基向量，`i -hat` 就是x轴基向量

我们称类似这样的变换改变了空间的定向。

**当空间定向改变的情况发生时，行列式为负。**

此时，行列式的绝对值依然表示区域面积的缩放比例。

> 比如说，我告诉你由(1,1)和(2,-1)为列的矩阵所代表的线性变换的行列式是-3.
>
> 这就是说变换后空间被翻转，并且面积放大为原来的3倍。



- **负的面积为什么和定向有关？**

> 这里强烈建议看3blue1brown原视频。
>
> 《线性代数的本质》行列式 5：05

当i帽靠近j帽时，空间也被压缩地更严重，这意味着行列式趋近于0；

当i帽与j帽完全重合时，行列式为0。

如果i帽继续沿着这个方向运动，行列式继续减小为负值将是一件很自然的事。

> 换一个角度看，二维的压缩，从三维视角看，就是一种旋转。
>
> 压缩时，向量`j `不动，向量 `i` 逐渐靠近 `j`。
>
> **想象 i 靠近 j ，越过 j 的过程：**
>
> 二维视角下，以i 和 j 为基 的坐标系接近时压缩，越过时拉伸。
>
> **三维视角下，把红向量j当做Z轴，想象绿向量在XOY平面上旋转——**
>
> **就像一个圆柱绕圆心的Z轴旋转，你只能看到 XoY 这一个侧切面的变化。**



- **三维空间，行列式意味着什么？**

依然是变换前后的缩放比例，不过这次缩放的是体积。

二维空间，我们考虑的是基向量i 和 j 组成的面积为1的正方形，并观察变换对它的影响。

三维空间，我们考虑的是基向量 i , j , k 组成的体积为1 的立体正方形。



**三维空间的行列式负值**：

有一种方法来描述三维空间的定向，那就是“右手定则——

- 右手食指指向i帽的方向；
- 伸出中指指向j帽的方向；
- 当你把大拇指竖起来时，它就正好指向k帽的方向。

如果线性变换后，你还可以用右手这么做，那么定向没有发生改变，行列式为正。

如果变换后，只能用左手描述了，那就是定向发生了改变，行列式为负。



**行列式的求值公式：** ad - bc 
$$
(a+b)(c+d)-ac-bd-2bc = ad-bc
$$
坐标系中， 小正方形凑成的网格面积 减去 平行四边形周围的三角形面积 = 平行四边形的面积



**行列式的一个性质证明：**
$$
det(M1M2) = det(m1) det(m2)
$$

> det行列式，M1、M2是矩阵

数值计算很麻烦。但从几何上理解：

**两个相继作用的总的线性变换对空间中几何维度造成的影响等于他们单独作用时造成影响的乘积。**

> 两次空间放缩引起的面积变化 是 两次单独放缩面积变化  倍数的乘积

在同一个变换下任何一个图形的拉伸倍率都是一样的，所以M1M2两次变换后的面积倍率结果是一致的。

> 注意：det乘det本质不是面积相乘，而是面积倍率相乘。

## 5.线性方程组

用一个式子概括：
$$
A\vec{x} = \vec{v}
$$

$$
\left(
\begin{matrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{matrix}
\right) 


\left[
\begin{matrix}
x_1 \\
x_2 \\
x_3 
\end{matrix}
\right] 

=

\left[
\begin{matrix}
a \\
b \\
c 
\end{matrix}
\right]
$$

> A 是未知数系数的矩阵，`x` 是未知数向量{x,y,z或x1,x2,x3......}， `v`是常数向量，如果是齐次的话，v 就是 很多 0 的 向量。



这个式子阐明了线性方程组问题中优美的几何直观部分：

**矩阵A代表一种线性变换，所以 求解Ax=v意味着我们去寻找一个向量x，使得它在变换后与v重合。**

> 这个时候逆矩阵的几何意义也很清晰了！

**当你逆向进行变换时，它实际上对应了另一个线性变换 —— 用`v`经过线性变换（矩阵）去寻找`x`。**

> 也就是等式两边同乘A逆，v逆向进行变换并跟踪x的动向。

这个线性变换就叫作A^-1^  A的逆矩阵！ 


> 比如说，如果A是逆时针旋转90度的变换,那么A的逆就是顺时针旋转90度的变换。
>
> 总的来说，A逆是满足以下性质的唯一变换：
>
> **首先应用A代表的变换，再应用A逆代表的变换，你会回到原始状态。**
>
> A逆乘以A等于一个“什么都不做”的矩阵，这个“什么都不做”的变换被称为“恒等变换”

**x和v重合有两种情况：**

1. 降维，两者变成一条线或一个点从而重合。
2. 不降维，在原有维度（二维、三维）上通过线性变换重合。

不降维，即存在唯一解的情况，这时存在A逆。

> A^-1^ 使得应用A变换再应用A逆变换之后，结果与恒等变换无异。

**降维即行列式为 0的情况；**

与这个方程组相关的变换将空间压缩到更低的维度上，此时没有逆变换，你不能将一条线“解压缩”为一个平面，也不能将一个点“回溯”成一条线（至少这不是一个函数能做的），你不能进行升维。

> 降维会损失信息。
>
> 空间坍缩为更低的维度后，变换信息有所丢失，无法得到其逆变换，也就是逆矩阵不存在

这样就会要求将一个单独的向量变换为一整条线的向量，但是函数只能将一个输入变换为一个输出。



**关于列空间（下面理解方程组求解要用）：**

**所有可能的变换结果(输出向量)的集合 被称为矩阵的“列空间。**

> 不管是一条直线、一个平面还是三维空间等。

矩阵的列告诉你基向量变换后的位置，这些**变换后的基向量张成的空间就是所有可能的变换结果 **;

换句话说**，列空间就是矩阵的列所张成的空间。**

> 列张成的空间 span of columns↔️列空间 Column space

**0向量一定包含在列空间中。**

> 因为线性变换要求原点位置不变。



结合这里的几何特性你可以理解：

对于线性方程组的求解情况——

- ==非齐次==

1. **增广矩阵r(A,v) 的秩和矩阵的秩r(A) 是否相等：**

   - `r(A,v) == r(A)`说明 **v 和A在一个维度上** ，至少有一个解

   - `r(A,v)  > r(A)` :说明 **v 比 A 的维度要高** ，无解

     > 列空间：列向量张成的空间`span`，忘记的看第 0 条 。
     >
     > 列空间是所有可能的 A 的列向量的线性组合构成的集合。
     >
     > 假设矩阵 A 是一个 m×n 的矩阵，那么它有 n 个列向量。这些列向量可以看作是 R^m^ (m维空间）中的向量。
     >
     > 如果这些列向量**线性无关**，它们可以张成一个 n 维的子空间，这个子空间就是 A 的列空间。
     >
     > 现在，当我们说向量 v 不在矩阵 A 的列空间中，这意味着没有一种方式通过 A 的列向量的线性组合来得到向量 v。
     >
     > **换句话说，v 向量的维度高于 A，于是不存在一个向量 x 使得 Ax = v 成立。**
     >
     > 如果我们将 A 的列向量看作是定义了一个平面或者高维空间中的一个超平面，那么 b 就是位于这个平面或超平面之外的点。

     r(A,v)  > r(A) 时，意味着 v 的维度比 A 的维度要高，A经过初等行变换会存在 全 0 行。

     >A如果线性相关，则必然存在 全 0 行；
     >
     >A如果线性无关，但比我们要追踪的 v 向量 少一行，也意味着A、v两者比较时，A下面要补充一个全0行。
     >
     >**总之就是少一个维度。**

      r(A) 里存在线性相关的列向量。于是 A 的线性变换相比 v向量 少了一个维度，找不到 x向量 经过低维线性变换 追踪到 v。

   

2. **秩的数量(A)是否和未知数n (x向量)一样**：

   - 如果一样，说明 **A 和  x 在同一维度**，x可以通过有限线性变化A  与 v重叠，**这时是唯一解。**

   - 如果不一样，说明 **A 比 x 维度低** ，x 想与 v 重叠，只能通过“降维”，**这时是无穷多解。**

     > 可以想象，物体投影到平面上只有一个影子；而从投影去猜测物体，物体可以有无限种形状。
     >
     > 同样地，经过降维压缩变换后，能够和低维的 v 重叠的 高维 x 有无穷多种。
     >
     > 例如三维→二维， 立体被压缩成一条线；
     >
     > 例如二维→一维，一条线被压缩成一个点。
     >
     > **都是无穷多解的情况。**



> 提问：增广矩阵的秩为n时，什么时候有解？
>
> 答：增广矩阵在n维度， A在n-1维度时无解。 只有当增广和A都在n维度时 才有解



- ==齐次==

  一定有解。

  > n 是 未知数个数。

  - r(A) = n ， **A 和 x 在同一维度** ，因为 v 向量是0，x 作为 0 向量，经过任何线性变换都和 v 重叠。

    > 是的，只有一个 唯一解，那就是0向量。

  - r(A) < n ,    **A 比 x 维度低** ，x 想与 0向量 重叠，只能通过线性变换来 “降维”，**这时是无穷多解。**

    > 这无穷多个解不是整个二维平面，而是二维平面/三维空间上的一条线，叫做零空间。





**于是不难理解行列式为 0 时的意义：**

行列式为0 时，意味着变换前后的面积倍率为0（压缩后面积为0），一定有维度变化。

对线性方程组，表现在齐次上就是无穷多解（线性变换降维），表现在非齐次上要么无解（v 比 A 维度高），要么也无穷多解(线性变换降维)。



## 6.秩 (Rank)

“秩”代表着变换后空间的维度
So the word **"rank"** means **the number of dimensions in the output of a transformation.**

> 列向量所张成空间的维度（基向量个数）

举例：

比如说对于2x2的矩阵，它的秩最大为2,意味着基向量仍旧能张成整个二维空间，并且矩阵的行列式不为零

但是对于3x3的矩阵，秩为2意味着空间被压缩了，但是和秩为1的情况相比，压缩并不是那么严重。

> 如果一个三维变换的行列式不为零，变换结果仍旧充满整个三维空间，那么它的秩仍为3。



**所以更精确的==秩的定义是列空间的维数==。**

当达到最大值时，意味着秩与列数相等，我们称之为“满秩”



**对一个满秩变换来说，唯一能在变换后落在原点的就是零向量自身。**

但是对一个非满秩的矩阵来说，它将空间压缩到一个更低的维度上。也就是说会有一系列向量在变换后成为零向量。

>举个例子，如果一个二维线性变换 将 非满秩（秩为1，两个基向量线性相关）平面压缩到一条直线上，那么沿某个不同方向直线上的所有向量就被压缩到原点。
>
>就好像柱子的侧面（直线）经过线性变换（旋转）变成了一个点（底面）。
>
>如果一个三维线性变换将空间压缩到一个二维平面上，同样也会有一整条线上的向量在变换后落在原点。
>
>压缩成二维直线也一样。

**零空间：**

变换后落在原点的向量的集合，被称为矩阵的“零空间”或“核。变换后一些向量落在零向量上，而“零空间”正是这些向量所构成的空间

> 比如上面例子中那一整条直线就是零空间。

**列空间的概念让我们清楚什么时候存在解，零空间的概念有助于我们理解所有可能的解的集合是什么样的。**

## 7. 非方阵矩阵

**举例分析**

- 3x2矩阵：是一个面，但仍然满秩。几何意义是将一个二维空间映射到三维空间上。

  > 二维空间输入 ，三维空间输出。

  矩阵有两列表明输入空间有两个基向量，有三行表明每一个基向量在变换后都用三个独立的坐标来描述。

- 2x3矩阵：

  > 三维空间输入 ，二维空间输出。

  几个基向量说明空间是几维；有两行表明这三个基向量在变换后都仅用两个坐标来描述，所以他们一定落在二维空间中。

**解线性方程**

- 3×2矩阵解线性方程：3个方程式，2个未知数，将二维空间投射到三维空间，要么无解，要么一个解
- 2×3矩阵解线性方程：2个方程式，3个未知数，将三维空间压缩成二维空间，要么无解，要么无穷多解

## 8.点乘和线性变换（相当劲爆）

**点乘标准形式：**
$$
\left[
\begin{matrix}
a \\ b
\end{matrix}
\right]
\cdot
\left[
\begin{matrix}
c \\ d 
\end{matrix}
\right]=

(a\cdot c)
+
(b\cdot d)
$$
**点乘几何形式：**

二维坐标系里进行演算可知，点乘就是其中一个向量在另一个向量上的投影长度，乘以被**投影**向量长度的成绩。

---

这一章的思想跨度较大（可以说很惊人、很震撼且富有美感），可以配合3b1b的图形化视频食用。

> 这里总结一下就是，用一个向量点乘另一个向量，实质上是做了一个线性变换。
>
> 点积可以看作是矩阵向量乘法的一个特例，是投影到一维的线性变换。
>
> 所以矩阵向量乘法的本质是线性变换。
>
> 线性变换的“物质载体“ ，则是其中一个向量扮演的。

**考虑一个问题**

==为什么点乘的运算过程：**对应坐标相乘并将结果相加，和投影有所联系？**==

让我们试着证明一下：

还是老样子，

1. 前提：**等距分布的点保持等距分布，从而为我们保证线性变换的基础。**

   > 否则线性变换，或者说高维向量在一维空间（数轴）上的投影就不是线性的，而是离散的。

2. 视角：这些线性变换完全由它对i帽和j帽的变换决定，所以只关注i帽、j帽。

   > 由 i帽 和 j帽 的变换，可以推出二维向量的变换。这也是以前对线性变换的分析方法，不拘泥于过程，由基向量推其他向量。
   >
   > **（u的坐标体现的是i,j向量的变化，而i,j向量就可以反映其余的改变）**



为什么要找i-hat和j-hat投影在斜数轴上？因为一开始那个把二维变一维的函数作的变换就是投影，而投影后的i-hat和j-hat的位置表示这种投影变换。

> i-hat 和 j-hat 的变换则代表了对应的所有列空间向量的变换。

定义一个从二维向量到数的线性变换，找到描述这个变换的1x2矩阵

> Projection matrix 投影矩阵 Transformation matrix 变换矩阵。

所以描述投影变换的1x2矩阵的两列，也就是 **i-hat和j-hat在斜数轴上的投影的值，根据对称性，正好分别是斜数轴u-hat的点的横纵坐标 u~x~ , u~y~ 。**

因此**空间中任意向量经过投影变换的结果，也就是投影矩阵与这个向量相乘**。

> 也就是投影矩阵[u~x~ , u~y~]与这个向量[i-hat, j-hat]相乘

这意味着，当一个二维向量v和另一个1x2的矩阵A 相乘时，**这和基向量与u帽的点积在计算上完全相同。** 

> 选择u帽长度为1并且落在一维数轴上，主要是为了体现后续u帽和i帽、j帽相互映射的对偶性



这就是为什么与单位向量的点积可以解读为将向量 v 投影到投影矩阵代表的单位向量 u 所在的直线上所得到的投影长度。

> 点积是一种将二维向量投影（降维）成一维向量（数轴）的线性变换。
>
> （高维向量在一维的投影）



矩阵向量相乘 Matrix-vector product <==> 点积 Dot product
$$
\left[
\begin{matrix}
u_x & u_y 
\end{matrix}
\right]

\left[
\begin{matrix}
x \\
y \\
\end{matrix}
\right]=
u_x \cdot x
+u_y \cdot y
$$
↑等价↓
$$
\left[
\begin{matrix}
u_x \\
u_y 
\end{matrix}
\right]

\left[
\begin{matrix}
x \\
y \\
\end{matrix}
\right]=
u_x \cdot x
+u_y \cdot y
$$


你在任何时候看到一个线性变换，它的输出空间是一维数轴。无论它是如何定义的，空间中会存在唯一的向量v与之相关，就这一意义而言，应用线性变换和与向量v做点积是一样的。

>它是数学中“对偶性”的一个实例。
>
>- 你可以说一个向量的对偶是由它定义的线性变换；
>
>- 一个多维空间到一维空间的线性变换的对偶是多维空间中的某个特定向量



**两个向量点乘，就是将其中一个向量转化为线性变换**

**有时你就会意识到，不把向量看作空间中的箭头，而把它看作线性变换的物质载体，会更容易理解向量**

向量就仿佛是一个特定变换的概念性记号。因为对我们来说，想象空间中的向量比想象整个空间移动到数轴上更加容易。



---

**举个例子：**
$$
\vec{x}\times\vec{v}=

\left[
\begin{matrix}
1 \\ -2 \\
\end{matrix}
\right]

\left[
\begin{matrix}
4 \\ 3 \\
\end{matrix}
\right]=

\left[
\begin{matrix}
1 & -2 
\end{matrix}
\right]

\left[
\begin{matrix}
4 \\ 3 \\
\end{matrix}
\right]=

4\times1
+
3\times(-2)
$$
将列向量 `[1 -2]` “横过来“ 看作是线性变换矩阵（Transformation matrix），此时，向量 v 的基向量 i-hat 和 j-hat 由默认的`[1,0]` ,`[0 1]` 经过线性变换 (乘变换矩阵) 变成了 `[1 0],[0 -2]` 。

> 这个线性变换的效果，实际上是“降维”，将两个二维基向量压缩成一条线（数轴）了，用点来表示，一个指向`+1` , 一个指向`-2`。

`[4,3]`这个向量跟随基向量一起线性变换（降维），变成了指向`-2`的向量。

> `4*(+1 0)` 表示 i-hat 长度变成4倍， `3 *(0 -2)`表示 j-hat 长度变 3 倍，叠加在一起就是 - 2 。

- 当你完全从数值角度进行计算时，它就是矩阵向量乘法，而点积可以看作是矩阵向量乘法的一个特例。
- 但从几何角度出发，向量`[1  -2]`转化为线性变换的载体，使另一个向量`[4 3]`的基向量变换了，于是 向量`[4 3]`的基向量线性变换后进行放缩（乘以对应倍率），得到的新向量，数值结果和向量乘法一样。



## 9.叉乘

**叉乘标准形式：**
$$
\left[
\begin{matrix}
a \\ b \\
\end{matrix}
\right]
\times
\left[
\begin{matrix}
c \\ d \\
\end{matrix}
\right]=

(a\times d)-(b\times c)

\
$$

$$
\vec{a}\times\vec{b}=

\left[
\begin{matrix}
a1 \\ a2 \\ a3
\end{matrix}
\right]
\times
\left[
\begin{matrix}
b1 \\ b2 \\ b3
\end{matrix}
\right]=

\left|
\begin{matrix}
\widehat{i} & \widehat{j} & \widehat{k}
\\ a1 & a2 & a3
\\ b1 & b2 & b3
\end{matrix}
\right|
$$

> i 的余子式 + j 的余子式 + k 的余子式

**叉乘几何意义：**

a~1~b~1~两个向量分别平移相交成 a~2~b~2~，四个向量a~1~a~2~b~1~b~2~围成的平行四边形的面积就是叉乘的绝对值。

正负性（定向问题）：以`a * b`为例。如果在二维平面上，a 在 b 右边（b需要经过右旋转到a），那么a*b就是正值。

如果a 在 b 左边（b需要经过左旋转到a），那么a*b就是负值。

> 注意：这是说顺序对叉乘有影响。

**几何角度观察定向：** i-hat 在 j-hat 右边，`i * j = +1` 。那么 a 在 b 右边，`a * b = +`

> 基向量的顺序就是定向的基础



---



**行列式的面积和叉乘有很大关系。**
$$
\left[
\begin{matrix}
a \\ b \\
\end{matrix}
\right]
\times
\left[
\begin{matrix}
c \\ d \\
\end{matrix}
\right]=

\left|
\begin{matrix}
a & c\\ b & d
\end{matrix}
\right|=

(a\times d)-(b\times c)

\
$$


`a,b`两个向量相乘 ，你只需要将 a向量作为行列式第一列，b向量作为行列式第二列，然后直接计算行列式。

所以：

1. 这里可以一个解释行列式性质：交换任意两行，行列式正负性改变。

   **几何上，两个向量叉乘交换顺序了，定向改变了，向量的基改变了，行列式为负不是很自然的吗？**

2. 你可能注意到一点，**当两个向量接近垂直时，他们的叉积最大。**因为此时构成的平行四边形面积最大。
3. **放大其中一个向量3倍，那么叉乘也增大3倍**。从面积上理解也很自然。

---

根据上面的二维向量叉乘，如果让我们猜测三维向量的叉乘，大概是：
$$
\left[
\begin{matrix}
u_1 \\ v_1 \\ w_1
\end{matrix}
\right]

\times

\left[
\begin{matrix}
u_2\\ v_2 \\ w_2
\end{matrix}
\right]

\times

\left[
\begin{matrix}
u_3 \\v_3 \\ w_3
\end{matrix}
\right]=

\left|
\begin{matrix}
u_1 & u_2  & u_3\\ v_1 & v_2 & v_2\\w_1 & w_2 & w_3
\end{matrix}
\right|
(这里是错误的猜想)
$$
输入三个向量，输出一个行列式（数值），这个行列式的数值是三维空间里，三个向量组成的体积。

> 虽然这么想很自然，却又**还不是真正的叉积**，但已经很接近了。

**真正的叉积是通过两个三维向量生成一个新的三维向量。**

叉积的结果不是一个数，而是一个向量。很奇怪吧？

这个**向量的长度就是平行四边形的面积**，而这个**向量的方向与平行四边形(所在的面)垂直。**

垂直方向有两个，至于是哪一个，要靠“**右手定则”：**

- 食指指向i帽的方向；
- 伸出中指指向j帽的方向；
- 当你把大拇指竖起来时，它就正好指向k帽的方向。

> 注意：第4节行列式也出现了右手定则。

==这里要证明一件事：**为什么叉积生成的新三维向量有这个性质？**==

**思路一：直接数值运算**

> 很直接很暴力，也很麻烦。这不是我们今天要讨论的。

**思路二：几何思路**

> 前提：**对偶性思想**——上一节点乘用到的思想。
>
> 每当你看到一个(多维)空间到数轴的线性变换时，它都与空间中的唯一个向量对应，也就是说应用线性变换和与这个向量点乘等价，数值上说，这是因为这类线性变换可以用一个只有一行的矩阵描述，而它的每一列给出了变换后基向量的位置。
>
> 这里的收获在于，每当你看到一个从空间到数轴的线性变换，你都能够找到一个向量，被称为**这个变换的对偶向量**（dual vector），使得应用线性变换和与对偶向量点乘等价。

叉积的运算同样体现了对偶性思想。

现在，我们将刚刚上面的猜想叉积的第一个向量u看作可变向量，比如(x,y,z)，而v和w保持不变，那么我们就有一个**从三维空间到数轴的函数**了。

> 这个函数根据v和w来定义（因变量），输入`(x,y,z)`（自变量）产生行列式（结果）。
>
> 你输入一个向量(x,y,z)，然后通过矩阵的行列式得到一个数，这个向量的第一列是(x, y,z)，其余两列是常向量v和w的坐标。

$$
f\left(
\left[
\begin{matrix}
x \\ y \\ z
\end{matrix}
\right]
\right)=
det
\left(
\left[
\begin{matrix}
x & v_1 & w_1 \\
y & v_2 & w_2 \\
z & v_3 & w_3
\end{matrix}
\right]
\right)
$$

$$
变量variable=
\vec{u}=
\left[
\begin{matrix}
x \\ y \\ z
\end{matrix}
\right]

& \vec{v} = 
\left[
\begin{matrix}
v_1 \\ v_2 \\ v_3 
\end{matrix}
\right]

& \vec{w} = 
\left[
\begin{matrix}
w_1 \\ w_2 \\ w_3
\end{matrix}
\right]
$$

> 这里将向量写作矩阵的列，而**教科书中大多将向量写作矩阵的行**。两种结果没有差异，因为转置不改变行列式的值。这里选择按列处理向量是为了更加直观。

这个函数的几何意义是，对于任一输入的向量(x, y,z), 考虑由它和v与w确定的**平行六面体,得到它的体积，然后根据定向确定符号。**

> **这个函数是线性的**，由行列式性质可知：
>
> 行列式就是三个向量所夹的平行六面体的体积。这个平行六面体的底面积是定下来的，所以如果高是线性的体积就是线性的。

一旦知道它是线性的，你就知道**可以通过矩阵乘法来描述**这个函数。

即**这个函数从三维空间到一维空间的每个线性变换，都存在一个1x3矩阵来代表这个变换。**	

> 对偶性告诉我们，你可以将这个1*3矩阵立起来，并且将整个变换看作与这个特定向量的点积。

$$
\overbrace{
\left[
\begin{matrix}
? \\ ? \\ ?
\end{matrix}
\right]
}^{神秘1\times3矩阵}

\left[
\begin{matrix}
x \\ y \\ z
\end{matrix}
\right]=


\overbrace{
\left[
\begin{matrix}
p_1 \\ p_2 \\ p_3
\end{matrix}
\right]
}^{这是\vec{p}}

\left[
\begin{matrix}
x \\ y \\ z
\end{matrix}
\right]=


det
\left(
\left[
\begin{matrix}
x & v_1 & w_1 \\
y & v_2 & w_2 \\
z & v_3 & w_3
\end{matrix}
\right]
\right)
$$

我们要找的就是这个特殊的三维向量——我称之为p向量。由上式可知，p与其他任一向量(x,y,z)的点积等于一个3x3矩阵的行列式。

**对于这个式子，有两种理解角度：计算和几何**

- 计算

  上式经过点乘：
  $$
  p1\cdot x+p2\cdot y+p3\cdot z =
  \begin{matrix}
  x(v2\cdot w3-v3\cdot w2)+ \\
  y(v3\cdot w1-v1\cdot w3)+ \\
  z(v1\cdot w2-v2\cdot w1)
  \end{matrix}
  $$
  所以很明显可以看出，我们要找的 p向量 的值就藏在右边的`v`和`w`坐标的线性组合中。
  $$
  \vec{p}=
  \left[
  \begin{matrix}
  p1 \\ p2 \\ p3
  \end{matrix}
  \right]=
  
  \begin{cases}
  p1 = v2\cdot w3-v3\cdot w2 \\
  p2=v3\cdot w1-v1\cdot w3 \\
  p3=v1\cdot w2-v2\cdot w1
  \end{cases}
  $$

  > 点积本是上是替换掉了原有的基向量，而叉积的行列式定义中有三个基向量正好可以用点积操作替换掉。

  像这样合并x、y和z前面的常数项，和把i帽、i帽和k帽放进矩阵第一列进行计算，然后合并各项前面的系数没有区别。

  > 把i帽、i帽和k帽放进矩阵第一列进行计算，正是老师根据教科书，叫我们死记硬背的叉积公式！

  **在矩阵中插入i帽、j帽和k帽不过是在传递一个信号，告诉我们应该把这些系数解读为一个向量的坐标。**

  

  好了，推导结束了。总结一下：

  **当你将向量p和某个向量`(x,y,z)`点乘时，所得结果等于一个由`(x,y,z)`和v与w确定的平行六面体的有向体积，数值上是一个3x3矩阵的行列式，这个行列式第一列为(x,y,z)，其余两列为v和w的坐标。**

  > 这是计算角度的总结。
  >
  > 让我们从几何角度看看，什么样的向量p才能满足这一特殊性质 ?

- 几何

  - 对于点积，向量p与其他向量的点积的几何解释，是将其他向量投影到p上，然后将投影长度与p的长度相乘。

  - 而对于叉积，上文中我们用一个线性函数来代表它。

    我们找到的线性函数对于给定向量的作用，也就是 p 作为一个 1x3矩阵，代表着一个线性变换；这个线性变换**将向量投影到垂直于v和w的直线上，然后将投影长度与v和w张成的平行四边形的面积相乘。**

    >这和`(x,y,z`) 与 `垂直于v和w且长度为平行四边形面积的向量`点乘是同一回事。

  首先获得由v和w确定的平行四边形的面积，乘以向量(x,y,z)在垂直于平行四边形方向上的分量(不是(x,y,z)的长度，是投影)，我们就可以得到这个平行六面体的体积，

  > 总结一下就是底面积(v,w)乘高(p投影)。

  更重要的是，如果你选择了合适的向量方向(点积为正)就会与(x,y,z)、v和w满足右手定则的情况相吻合。

  

  **这意味着我们找到了这个p，使得p与和某个向量(x,y,z)点乘时，所得结果等于和上面计算得来的3x3矩阵的行列式是一致的。**

  这就是我们要找的，计算和几何得出的同一个向量 p 。



## 10.基变换

同一个向量，在基向量不同的坐标系，会有不同的表达方法。这就像不同的语言，虽然我们都在关注空间中的同一个向量，但是我们可以用不同的语言和数字来描述它。不同的基向量，坐标轴的方向与网格间距会有所不同。

于是一个很自然的问题：==如何在不同坐标系之间进行转化?==

假设你的朋友詹妮弗自己建立了一个坐标系，第一个基向量b1 = (2,1) , 第二个基向量b2=  (-1,1)

>在她的坐标系中，这两个向量的坐标为(1,0)和(0,1)

对于向量[-1 2] ，她的坐标系里，这个向量是-1乘以b1加上2乘以b2。

从我们的角度来看，b1的坐标为(2，1)，b2的坐标为(-1，1)，所以实际上，我们可以直接计算-1乘以b1加上2乘以b2，因为它们都是在我们的坐标系中表示的。计算后得到的是[-4 1] 的向量。

这里发生的过程，也就是**用某个向量的特定坐标与她的基向量数乘，不就是矩阵向量乘法吗？**

> 这个矩阵的列代表的是用我们的语言表达的詹妮弗的基向量。

一个矩阵的列为詹妮弗的基向量,这个矩阵可以看作一个线性变换。

> 它将我们的基向量i帽和j帽，也就是我们眼中的(1, 0)和(0,1)变换为詹妮弗的基向量，也就是她眼中的(1,0)和(0，1)

==这里有一个疑惑：==
$$
Our \; grid \longrightarrow Jennifer's\;grid \\
\left(
\begin{matrix}
2 & -1 \\ 1 & 1
\end{matrix}
\right)
\\
Jennifer's\;language \longleftarrow Our \; language
$$
从几何上说，这个矩阵将我们的网格变换为詹妮弗的网格,

但是从数值上说，这是用她的语言来描述转化为用我们的语言来描述。

让我们恍然大悟的是，把它看作将我们对詹妮弗的向量的误解，也就是**在我们的坐标系中具有相同坐标的向量，变成她真正想要表示的向量。**

==那么反方向呢？==

我们的坐标系中的`(3，2)`的向量，怎么得到詹妮弗坐标系中的`(5\3, 1\3)`

之前的基变换矩阵是从詹妮弗的语言转化到我们的语言，那**我们对矩阵取个逆不就好了 ！**

> 记住一点，一个变换的逆是一个新的变换,他讲对应的变换逆向进行。

以上就是如何在坐标系之间对单个向量的描述进行相互转化。

**总结**：一个矩阵的列代表的是詹妮弗的基向量，却是用我们的坐标来描述；对于一个向量，这个矩阵将她的语言描述转化为我们的语言描述。逆矩阵则相反。

---

**向量并不是唯一用坐标表示的东西。线性变换同样要用坐标向量表示。**

> 这就涉及到了基向量问题(看待坐标的视角)

考虑某个线性变换，譬如逆时针旋转90你我用矩阵代表它的时候，我们是在跟踪i帽和j帽的去向。

>i帽在变换后处于坐标(0，1)  而j帽在变换后处于坐标(-1，0)。
>
>这些坐标也就成为了矩阵的列

$$
\left(
\begin{matrix}
0 & 1 \\ -1 & 0
\end{matrix}
\right)
$$

但是这种表示与我们对基向量的选择密切相关。因为我们跟踪的是i帽和j帽,并且是在我们自己的坐标系中记录它们的去向。

==而詹妮弗如何描述同样的空间90°旋转呢?==

你可能会尝试只将旋转矩阵的列转化为用詹妮弗的语言描述——因为这些列代表的是i帽和j帽的去向。

但是这些列代表的仍然是我们的基不是她的基。而詹妮弗想要的矩阵需要代表她的基向量的去向,也就是用她的语言描述。
$$
\overbrace{
\left[
\begin{matrix}
-1 \\ 2
\end{matrix}
\right]
}^{Vector\;in
\;Jannifer's\;language}

\longrightarrow

\overbrace{
\underbrace{
\left[
\begin{matrix}
2 & -1 \\ 1 & 1
\end{matrix}
\right]
}_{基变换矩阵}

\left[
\begin{matrix}
-1 \\ 2
\end{matrix}
\right]
}^{Same\;vector\;in\;our\;langugae}

\longrightarrow

\quad
\overbrace{
\underbrace{
\left[
\begin{matrix}
0 & -1 \\ 1 & 0
\end{matrix}
\right]
}_{变换矩阵\;in\;our\;language}

\left[
\begin{matrix}
2 & -1 \\ 1 & 1
\end{matrix}
\right]

\left[
\begin{matrix}
-1 \\ 2
\end{matrix}
\right]
}^{变换后向量\;in\;our\;language}
$$

$$
\left[
\begin{matrix}
0 & -1 \\ 1 & 0
\end{matrix}
\right]

\left[
\begin{matrix}
2 & -1 \\ 1 & 1
\end{matrix}
\right]

\left[
\begin{matrix}
-1 \\ 2
\end{matrix}
\right]

\longrightarrow

\overbrace{
\underbrace{
\left[
\begin{matrix}
2 & -1 \\ 1 & 1
\end{matrix}
\right]^{-1}
}_{基变换矩阵的逆}
\left[
\begin{matrix}
0 & -1 \\ 1 & 0
\end{matrix}
\right]

\left[
\begin{matrix}
2 & -1 \\ 1 & 1
\end{matrix}
\right]

\left[
\begin{matrix}
-1 \\ 2
\end{matrix}
\right]
}^{变换后向量\;in\;Jannifer's\;language}
$$

因为我们能够对詹妮弗语言描述的任一向量做同样的事: 

==基变换 → 线性变换 → 基变换的逆==

**这三个矩阵的复合给出的就是用詹妮弗语言描述的线性变换矩阵**。

它接收用詹妮弗语言描述的向量，并输出用詹妮弗语言描述的变换后的向量。

所以，如果詹妮弗用这个矩阵与她的坐标系中的一个向量相乘，结果就是在她的坐标系中描述的该向量旋转90°的结果。

总结：每当你看到这样一个表达式:A逆乘以M乘以A,这就**暗示着一种数学上的转移作用**。

> 中间的矩阵代表一种你所见的变换，而外侧两个矩阵代表着转移作用，也就是视角上的转化。
>
> **矩阵乘积仍然代表着同一个变换，只不过是从其他人的角度来看的。**

**基变换两个应用实例：**

1.特征值与特征向量

2.相似对角化。



## 11.特征值和特征向量

- ==引言==

首先，考虑二维空间中的某个线性变换：
$$
\left[
\begin{matrix}
3 & 1 \\
0 & 2
\end{matrix}
\right]
$$

>它将基向量i帽变换到坐标(3,0)，j帽变换到坐标(1,2)

我们关注它对一个特定向量的作用，并且考虑这个向量张成的空间，也就是通过原点和向量尖端的**直线**。
大部分向量在变换中都离开了其张成的空间；

> 所以如果向量正好落在这条直线上，感觉更像是巧合。不过，某些特殊向量的确留在它们张成的空间里，意味着矩阵对它的作用仅仅是拉伸或者压缩而已，如同一个标量。

在这个例子中，基向量i帽就是这样一个特殊向量。i帽张成的空间是x轴，i帽变成了原来的3倍，仍然留在x轴上。

> 此外，因为线性变换的性质，x轴上的任何其他向量都只是被拉伸为原来的3倍，因此也就留在它们张成的空间里。

有一个略显隐蔽的向量(-1,1)，它在变换中也留在自己张成的空间里，最终被拉伸为原来的2倍；

> 同上，线性性质暗示着，处在它所张成的对角线上的其他任何一个向量，也仅仅被拉伸为原来的2倍。

**这些特殊向量就被称为特征向量。每个特征向量都有一个所属的值，被称为“特征值”, 即衡量特征向量在变换中拉伸或压缩比例的因子。**

如果特征向量是负值1/2 ，意味着这个向量被反向，并且被压缩为原来的1/2。

**重点在于，它仍然停留在它张成的直线上，并未随着线性变换发生旋转。**

> 特征向量的一个应用：三维空间旋转。
>
> 例如一个立方体，旋转的话，如果你能找到这个旋转的特征向量，那么你找到的就是旋转轴。这种情况下，相应的特征值必须为1，因为旋转不缩放任何一个向量。

旋转的应用告诉我们：

对于任一矩阵描述的线性变换，你可以通过将矩阵的列看作变换后的基向量来理解它。但今天我们学习了特征向量，那么在不依赖于特定坐标系（基向量变化）的前提下，一种更较好且常用的的理解方式就是特征向量和特征值。

> 对于一个矩阵A有两种理解方式 一种理解方式是基于坐标系的，认为矩阵的每一个列向量是线性变换后的基向量；另一种理解方式使用特征向量和特征值。

- ==概念==

$$
\overbrace{A}^{变换矩阵}\;\vec{v}=\lambda\overbrace{\vec{v}}^{特征值} \\
\quad\searrow\quad\swarrow\\
\quad\;特征向量
$$

> `矩阵向量乘积 = 向量数乘` 。因此求解矩阵A的特征向量和特征值，实际上就是求解使得这个等式成立的向量v和数λ。

想理解这个式子，首先要解决乘积类型不同的问题。

所以我们首先将等号右侧重写为某个矩阵向量乘积，让 λ 当成某个矩阵。

> 其中，矩阵的作用效果是将任一向量乘以 λ，这个矩阵的列代表着变换后的基向量，而每个基向量仅仅与λ相乘
> 所以这个矩阵的对角元均为λ，其余位置都是0。

无非是 λE，乘以一个单位矩阵E。

我们就能将等号右侧的东西移到左侧，然后提出因子v
$$
(A-\lambda I)\vec{v} = \vec{0}
$$
左括号里的矩阵类似于下面这种矩阵
$$
\left[ 
\begin{matrix}
3-\lambda & 1 & 4\\
1 & 5-\lambda & 9\\
2 & 6 & 5-\lambda
\end{matrix}
\right]
$$
于是我们可以寻找一个向量v，使得这个新矩阵与v相乘结果为零向量。

1. **首先是v自己是零向量**。这时等式恒成立。

   > 这种情况比较无聊，看下面。

2. **v不是零向量**。这时考虑前面学到的内容，当且仅当矩阵代表的线性变换**将空间压缩到更低的维度时**，才会存在一个非零向量使得矩阵和它的乘积为零向量。

   > 简单说，非**零向量是有长度滴。线性变换想让它变成一个点，只能通过降维压缩。**

   **那么考考大家，什么特性的矩阵能降维压缩？——行列式为0的矩阵（压缩完面积为0）。**

   观察括号式子代表的矩阵，有一个变量λ。在上面那个例子中，λ=1时行列式为 0。

   > 当然，对于其他矩阵，特征值不一定是1，λ取其他值时才能使行列式为零。
   >
   > 这里3b1b的原视频有一个随着λ变化，向量逐渐线性变换的过程，很精彩！

   这个 λ 值保证了概念式子的成立，也就说明此时向量v是A的一个特征向量，在变换中停留在它张成的空间里。

   >在这个例子中，v对应的特征值是1，所以它实际上保持不变

**举例说明：**
$$
det
\left(
\left[
\begin{matrix}
3 & 1 \\
0 & 2
\end{matrix}
\right]
\right)=
(3-\lambda)(2-\lambda)=0
$$
还是开头这个例子，求解它的特征值，将对角元减去λ，然后计算行列式。这样我们就得到了一个关于入的二次多项式`(3-入)(2-入)`。由上面的推论可知，只有行列式为零时，λ 才会是特征值。所以这个矩阵的λ = 2 或 λ = 3。
$$
\left[
\begin{matrix}
3-\lambda & 1 \\
0 & 2-\lambda
\end{matrix}
\right]
\left[
\begin{matrix}
x \\ y
\end{matrix}
\right]=
\left[
\begin{matrix}
0 \\ 0 
\end{matrix}
\right]
$$
为了求出属于特征值的特征向量，将 λ 代入到矩阵中，然后求解出在能经过个矩阵的线性变换后成为零的向量。于是得到`x+y = 0`

> 你会发现所有的解全部落在由向量(-1,1)张成的对角线上

与之对应的，就是原始的矩阵[(3,0)，(1,2)]将这些向量拉伸为原来的2倍。

---

==三个特征向量的特殊情况==

**1.二维线性变换不一定有特征向量**

比如正交变换：
$$
\left[
\begin{matrix}
0 & -1 \\
-1 & 0
\end{matrix}
\right]
$$
它并没有特征向量，因为每一个向量都发生了旋转并离开了其张成的空间。硬去求特征值，会得到 λ^2^+1=0 。有两个虚数解而没有实数解。没有实数解表明它没有特征向量。

> 扩展：但虚数和复平面有关系。与1相乘在复平面中表现为90度旋转和i是这个二维实向量旋转变换的特征值有所关联。特征值出现复数的情况般对应于变换中的某种旋转大。
>
> 具体细节这里不深究了。

**2.剪切变换：**
$$
\left[
\begin{matrix}
1 & 1 \\
0 & 1
\end{matrix}
\right]
$$
所有x轴上的向量都是属于特征值1的特征向量，因为它们都保持不变

当你将对角元减去入，然后计算行列式,你得到的是 (1-λ)^2^  —— λ 是重根。

> 几重根就表明最多对应几个线性无关的特征向量

这与几何上得到的“所有特征向量均属于特征值1”的结果一致。

**3.可能会出现只有一个特征值，但是特征向量不止在一条直线上的情况**:
$$
\left[
\begin{matrix}
1 & 1 \\
0 & 1
\end{matrix}
\right]
$$
一个简单的例子是将所有向量变为两倍的矩阵。

唯一的特征值是2，但是平面内**每一个向量都是属于这个特征值的特征向量。**

---

==如果的基向量都是特征向量，会发生什么？==
$$
\left[
\begin{matrix}
-1 & 0 \\
0 & 2
\end{matrix}
\right]
$$
比如说，可能i帽变为原来的(-1)倍，j帽变为原来的2倍，将它们的新坐标作为矩阵的列。

注意，它们的倍数-1和2，也就是i帽和j帽所属的特征值，位于矩阵的对角线上，而其他元素均为0。

像这种除了对角元以外其他元素均为0的矩阵被称为对角矩阵,

解读它的方法是，**所有基向量都是特征向量。矩阵的对角元是它们所属的特征值**

**对角矩阵有很多有趣的性质：**

其中一个重要的方面是，**对角矩阵与自己多次相乘的结果更容易计算**

> 因为对角短阵仅仅让基向量与某个特征值相乘,所以多次应用矩阵乘法，比如100次，也只是将每个基向量与对应特征值的100次幂相乘。

但这种基向量是特征向量的情况，你可能很难遇到。

如果你的变换有许多特征向量，就像上面有两条的矩阵，多到你能选出一个张成全空间的集合，那么你就能变换你的坐标系，使得这些特征向量就是基向量

> 空间有几维，就找出几个线性无关的特征向量。

所以又想到相似对角化了吗？矩阵A能对角化的充要条件是A有n个线性无关的特征向量!!!

> 这里还用到了上一节的基变换。



基变换矩阵是特征向量构成的，而特征向量是当前空间进行中间矩阵变换时不空间不变化的向量。

所以**把当前空间的变换转换到特征基的空间后，变换就变成了对基的放缩操作，也就是特征值构成的矩阵**

> 用特征向量作为基向量，变换只对基作伸缩。所以这个新矩阵必然是对角的，并且对角元为对应的特征值

所以一组基向量(同样是特征向量)构成的集合被称为一组“特征基”

所以如果你要计算这个矩阵的100次幂，更容易的做法是先变换到特征基，在那个坐标系中计算100次幂。然后转换回标准坐标系。

> 不是所有变换都能进行这一过程。比如说剪切变换，它的特征向量不够多，并不能张成全空间。但是如果你能找到一组特征基，矩阵运算就会变得非常轻松



## 12.抽象向量空间

向量根本上并不是由一组实数构成，它们的本质其实更具空间性。

从某种意义上说，函数实际上只是另一种向量。函数既满足向量的可加性，又满足成比例。而对向量所能进行的操作不过相加和数乘两种。

所以，最初以空间中的箭头为背景考虑的线性代数的合理概念和解决问题的手段，应该能够原封不动地被我们取出来用于函数。

比如：

- ==线性变换==：函数的线性变换接收一个函数，我并把它变成另一个函数。微积分中的==导数==满足这个性质。

  > 关于这点，有时你听到的是“算子”而不是“变换” 。（线性算子）

  **注意**：导数既满足可加，又满足成比例。所以**求导是一种线性运算。**

- ==基向量==：函数空间倾向于无穷维，整个空间包含了任意高次的多项式。

  > 我们见到的多项式往往是有限项。

  我们要做的是给这个空间赋予坐标的含义，这需要选取一个基。因为多项式已经是数乘x的不同次幂再做加和的形式，所以我们很自然地**取x的不同次幂作为基函数。**

  **基函数举例：**
  $$
  1x^2+3x+5
  $$
  第一个基函数是常函数：b~0~(x) = 1 (5*1=5)

  第二个基函数是b~1~(x) = x

  第三个基函数是b~2~(x) = x^2^

  ......

  基函数在这里起到的作用，和i帽、j帽和k帽在向量(箭头)的世界中起到的作用类似。

  > 因为多项式的次数可以任意高，所以这个基函数集也是无穷大的。

  上述多项式的坐标就是 5、3、1、0、0、0......

  多项式4x^5^-5x^2^的坐标就是0、0、-5、0、0、4，然后加上一串无限长的0。

  **总的来说，因为每一个多项式都只有有限项，所以它的坐标就是有限长的一串数，再跟上无限长的一串零。**

  

- ==矩阵求导==：在这个坐标系中，求导是用一个无限阶矩阵描述的。

  你可以用这种方法构建这个矩阵:求每一个基函数的导数，然后把结果放在对应列。
  $$
  \begin{matrix}
  b_0(x)=1\\
  b_1(x)=x\\			
  b_2(x)=x_2\\
  b_3(x)=x_3\\
  \end{matrix}
  \qquad
  \frac{d}{dx}b_0(x)=\frac{d}{dx}(1)=
  \left[
  \begin{matrix}
  0 \\ 0 \\ 0 \\0
  \end{matrix}
  \right]
  \qquad
  \frac{d}{dx}b_1(x)=\frac{d}{dx}(x)=
  \left[
  \begin{matrix}
  1 \\ 0 \\ 0 \\0
  \end{matrix}
  \right]
  \qquad
  $$

  $$
  \frac{d}{dx}b_2(x)=\frac{d}{dx}(x^2)=2x=
  \left[
  \begin{matrix}
  0 \\ 2 \\ 0 \\0
  \end{matrix}
  \right]
  \qquad
  \frac{d}{dx}b_3(x)=\frac{d}{dx}(x^3)=2x=
  \left[
  \begin{matrix}
  0 \\ 0 \\ 3 \\0
  \end{matrix}
  \right]
  \qquad
  \frac{d}{dx}b_4(x)=\frac{d}{dx}(x^4)=2x=
  \left[
  \begin{matrix}
  0 \\ 0 \\ 0 \\4
  \end{matrix}
  \right]
  $$

  最终得到一个矩阵：
  $$
  \left[
  \begin{matrix}\
  0 &1 & 0 & 0 & 0\\
  0 &0 & 2 & 0 & 0\\
  0 &0 & 0 & 3 & 0\\
  0 &0 & 0 & 0 & 4
  
  \end{matrix}
  \right]
  $$

**总结：**

乍一看，矩阵向量乘法和求导像是毫不相干的。

| 线性代数                                | 函数                          |
| --------------------------------------- | ----------------------------- |
| Linear transformations 线性变换   <br/> | Linear operators线性算子<br/> |
| Dot products 点积                       | Inner products 内积           |
| Eigenvectors 特征向量                   | Eigenfunctions特征函数        |

- 向量空间：

我想在这里指出的是，**数学中有很多类似向量的事物，只要你处理的对象集具有合理的数乘和相加概念。**

不管是空间中的**箭头、一组数、函数**的集合，还是你定义的其他奇怪东西的集合。线性代数中所有关于向量、线性变换和其他的概念都应该适用于它。**这些类似向量的事物，比如箭头、一组数、函数等，它们构成的集合被称为“向量空间'**

针对向量空间，数学家们建立了一系列向量加法和数乘必须遵守的规则，这些规则称为“公理”。

**只要你定义的新家伙满足这些规则，那你就可以称它为向量，并运用我们学到的那些线性变换之类的工具。**

> 在线性代数的现代理论中，果要让所有已经建立好的理论和概念适用于一个向量空间，那么它必须满足八条公理。如果要让所有已经建立好的理论和概念适用于一个向量空间，那么它必须满足这八条公理，类似“入场券”似的东西。

这些公理并非基础的自然法则，它们是一个媒介。一边连接着你，也就是发现这些结论的数学家，另一边连接着其他人，也就是想要把这些结论应用于新的向量空间的人。只要向量的定义满足这些公理，就能顺利地应用我们发现的的所有关于向量的结论。

- 向量不止是某个“熟面孔”

  这就是为什么你阅读的每一本教科书都会根据可加性和成比例来定义线性变换，而不是用网格线保持平行且等距分布什么的来定义。即便后者更加直观。

  **向量的形势并不重要。**只要尚量相加和数乘的概念遵守以上规则即可。

  > **所以对于“向量是什么”这个问题，数学家会直接忽略不作答。**就好像问 3 这个抽象数字是什么含义一样。

  因此，你往往会把你的所有结论抽象地表述出来，也就是说仅仅根据这些公理表述。而不是集中于某一种特定的向量上，
  像是空间中的箭头或者函数等

  

## 13.Cramer法则(克拉默/克莱姆)

计算线性方程组，往往用高斯消元法，而不用克拉默法则。但了解它的几何意义会帮你加深对线性方程组的理解。



一个线性方程组的小例子；
$$
\left\{
\begin{aligned}
3x+2y=-4 \\
-x+2y=-2
\end{aligned}\right.
$$


**可以把这个方程组看作对`[x y]`向量的一个已知的矩阵变换:**而且变换后的结果已知 在这里是[-4;-2]
$$
\left[
\begin{matrix}
3 & 2\\
-1 & 2
\end{matrix}
\right]
\left[
\begin{matrix}
x \\ y
\end{matrix}
\right]=
\left[
\begin{matrix}
-4 \\ -2
\end{matrix}
\right]
$$

> 注意：矩阵的列向量反映了**矩阵是如何变换**的，**分别告诉了你基向量变换后的位置**——也就是求[2,2] [3,-1]坐标系中的[-4，-2]对应到直角坐标系中是多少

所以问题变成了 哪个输入向量[x,y] 在变换后会成为[-4;-2]。

> 由5.线性方程组那一节，**矩阵A代表一种线性变换，所以 求解Ax=v意味着我们去寻找一个向量x，使得它在变换后与v重合。**

这里有两种结果，取决于矩阵变换是否降维（行列式是否为0）。

- 如果行列式为0，意味着要么任何输入向量都不会变换到给定的输出向量，要么有无数个向量都会变换到给定的向量。

  > 以二维空间为例。前一种是二维输入向量的空间压缩成了一条线，而输出向量在线之外；后一种是输出向量恰好在线内，所以所有输入向量都满足。

- 行列式不为0是我们今天重点探讨的，意味着线性变换后维数依然相同。

  每一个输入向量都对应一个输出向量，反之亦然。

> 下面是行列式不为0（不降维）的情况。

来看一个猜想：
$$
\left[
\begin{matrix}
x\\y 
\end{matrix}
\right]
\left[
\begin{matrix}
1 \\ 0
\end{matrix}
\right]=x
\longrightarrow
T\left(
\left[
\begin{matrix}
x\\y 
\end{matrix}
\right]
\right)
\cdot
T\left(
\left[
\begin{matrix}
1\\0
\end{matrix}
\right]
\right)=x
\\
\left[
\begin{matrix}
x\\y 
\end{matrix}
\right]
\left[
\begin{matrix}
0\\1
\end{matrix}
\right]=y
\longrightarrow
T\left(
\left[
\begin{matrix}
x\\y 
\end{matrix}
\right]
\right)
\cdot
T\left(
\left[
\begin{matrix}
0\\1
\end{matrix}
\right]
\right)=y
\\
T(向量)是指向量线性变换后
$$
如果这个情况合理的话，我们能很轻松算出[x;y]的解。因为目标向量`[-4 -2]` ，两个基向量线性变换后的状态(矩阵)都是已知的。

**可惜这个猜想是错误的。**==对大多数线性变换来说 点积会随着变换而改变。==

> 比如说，有两个向量大致指向同一个方向，点积为正，变换后被拉远，像这样它们的点积就变成负数了。类似的
> 就算两向量相互垂直 点积为零 比如两个基向量，我们也无法保证它们在变换后依然相互垂直。

那些不改变点积的矩阵变换，有一个特殊的名字：**正交变换**。

它们使基向量在变换后依然保持单位长度、且相互垂直，可以想成是旋转矩阵， 相当于作刚体运动没有拉伸压缩或变形。

在正交变换中求解非常简单。**因为点积不变，所以已知的输出向量和矩阵的列向量的点积，分别等同于未知输入向量和各个基向量的点积。**

举例：
$$
\left[
\begin{matrix}
cos(30°) & -sin(30°)\\
sin(30°) &  cos(30°)
\end{matrix}
\right]
\left[
\begin{matrix}
x \\ y
\end{matrix}
\right]=
\left[
\begin{matrix}
1 \\ 2 
\end{matrix}
\right]\\
x=
\left[
\begin{matrix}
1 \\ 2 
\end{matrix}
\right]\cdot
\left[
\begin{matrix}
cos(30°) \\ sin(30°)\\
\end{matrix}
\right]
\qquad
y=
\left[
\begin{matrix}
1 \\ 2 
\end{matrix}
\right]\cdot
\left[
\begin{matrix}
-sin(30°) \\  cos(30°)
\end{matrix}
\right]
$$
虽然这个思路对大多数线性方程组都不成立，但它给了我们一个方向去思考，有没有另一种对输出向量坐标值的几何解释。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3f866f3c3b8640e8a7c845be811cf603.png#pic_center)

这个由第一个基向量i和未知的输入向量「x;y]组成的平行四边形。面积是x轴基向量i 乘以 输入向量 的y值。

于是==我们可以用这个平行四边形的面积来表示y值。==

> 当然面积是有向的。如果向量的v坐标为负，则四边形面积也为负。
>
> （前提是你把基向量i放在第一位来定义平行四边形）

同样观察由未知的输入向量，和第二基向量j组成的平行四边形。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f2af9781d6254591b4c525479b38d5e5.png#pic_center)



以此类比：看三维空间。

> 对于向量的z轴坐标，可能你的第一反应是向量在z轴上投影的坐标值，是它与第三个基向量k的点。但......

更好的方法还是**考虑基向量**：考虑向量与另外两个基向量i和i所组成的平行六面体，底面是由基向量i和i组成的正方形，面积是1。

==所以它的体积值等同与它的高 也就是我们这个向量的z坐标==

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b89e8fd1271c43cfbd59698767e6ce7f.png#pic_center)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/fc0c3398472440dcba92c362047f2c74.png#pic_center)

同样的，用这个奇怪的方法来描述向量在某一个轴上的坐标值，**可以先考虑向量本身，和除这个轴之外的两个基向量组成的平行六面体，然后其体积就是对应的坐标值。**

> 另外 我们可以讨论平行六面体体积的==有向性==，就是之前在行列式视频中提到的右手法则。**于是列向量的顺序就很重要了，坐标的正负性也有了意义。**
>
> 注意考虑有向面积，这对于我们拓展到高维的计算很有用，也就会明白“为什么变换后行列式是这么算的。

$$
\mathbf{x}=det
\left(
\left[
\begin{matrix}
\mathbf{x} & 0 & 0 \\
y & 1 & 0 \\
z & 0 & 1
\end{matrix}
\right]
\right)
\qquad
\mathbf{y}=det
\left(
\left[
\begin{matrix}
x & 0 & 0 \\
\mathbf{y} & 1 & 0 \\
z & 0 & 1
\end{matrix}
\right]
\right)
\qquad
\mathbf{z}=det
\left(
\left[
\begin{matrix}
x & 0 & 0 \\
y & 1 & 0 \\
\mathbf{z} & 0 & 1
\end{matrix}
\right]
\right)
$$

> **注意加粗。**

好了 为什么要把坐标值和面积或体积联系起来呢？因为当你**做矩阵变换后，平行四边形的面积不一定保持不变，可能成比例增大或减小。**

==成比例放缩意味着，我们要引入行列式了——==

>由4.行列式那一节：**行列式代表测量一个给定区域面积增大或减小的比例**

举个例子(一个新的平行四边形)：
$$
\left[
\begin{matrix}
2 & -1\\
0 & 1
\end{matrix}
\right]
\left[
\begin{matrix}
x \\ y
\end{matrix}
\right]=
\left[
\begin{matrix}
4 \\ 2
\end{matrix}
\right]
$$
比如 考虑一个新的平行四边形，第一条边是变换后的第一基向量(也就是矩阵的第一列)`[2 0`]。第二条边是变换后的`[x,y]` 那它的面积是多大呢？

> 变换前的面积是未知输入向量的y坐标值，所以变换后的面积等于**矩阵的行列式**乘以y值

所以可以用输出的平行四边形面积，除以矩阵的行列式计算出y。
$$
y = \frac{area}{det(A)}
$$
==那怎么求变换后的面积area呢？==

因为这是一个线性方程组，我们知道线性变换后的向量`[4 2]`。

**既然我们已知最终变换后的向量,那么 可以构造一个新矩阵** 第一列和我们原先的矩阵相同`[2 0]`,而第二列是输出向量。 然后取这个新矩阵的行列式——

你看，我们只需使用到变换后的两个向量，也就是矩阵的列向量们和已知输出问量，就能计算得出未知输入向量的y值。方程已经解好一半了。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/18ac7c32941c41b1933a6939dc77c96c.png#pic_center)
$$
area = 
\left(
\left[
\begin{matrix}
2 & 4 \\
0 & 2 
\end{matrix}
\right]
\right)
\qquad
y = \frac{area}{det(A)}
$$
我们可以用同样方法得到 x 值。

我们之前定义的平行四边形回头来看。

(有向)面积为输入向量的x值 由未知向量和j基向量构成，而变换后的它 由输出向量和矩阵的第二列组成。面积增大的比例是矩阵的行列式，**所以输入向量的x值是新的面积除以变换矩阵的行列式。**

于是可以创造一个新矩阵来计算变换后的平行四边形的面积，这个新矩阵第一列为输出向量 第二列和变换矩阵相同。

所以和上面的错误猜想一样，**我们仅使用到输出部分的数值**(这些我们在最初线性方程组里已知的数值) ， **就能解出x值。**是不是很神奇！

**这个线性方程组的解法 被称为克莱姆法则**