**前言：**

> 一个很久之前截的B站评论:
>
> @？
>
> 很多人学不懂线代(高代)的原因是，线性代数这门课本质上来说是 具体-抽象-再具体 的回旋过程首先从线性方程组开始讲起，讨论矩阵，方阵的性质然后再由矩阵上升到线性空间上的线性映射线性变换，在研究线性映射和线性变换时又会通过和数域K的联系再具体化到矩阵所以入门学习矩阵的时候，很容易迷失在处处严谨的证明细枝未节里，从而没法把握核心的思路与研究目的;
>
> 而矩阵没学好，后面的线性空间更难理解，因为线性空间研究往往是再具体化到矩阵上来进行的
>
> 所以我建议朋友们学习高代(线代)的时候，在初期就把这门课当作一门外语来学在矩阵部分对每一条定理每一个命题都把他当作单词吃透当你带着扎实的矩阵功底去接触线性空间的时候，马上如拨云见日般。你也会很快感受到代数学具体抽象再具体的魅力。
>
> @吸猫群众QAQ:
>
> 数无形时少直觉，形少数时难入微，数形结合百般好。越学习越惊叹于古人的智慧，三百多年的科学史，孕育出了多少绝妙的思想。想到了年纪轻轻却在数学体系不完善的年代创造出微积分，用古老的几何证明法得出若干现在仍被奉为真理的几大定律，却在晚年困惑于神与世界本质的思辨中的牛顿;还有靠现有的数学工具以及其绝妙的思维创造出广义相对论，以一人之力颠覆了人类对空间和时间这两个玄妙概念的理解，在晚年试图创造出可以解释世界的终极定理却终究敌不过时间的爱因斯坦;还有那些出于对一个奇怪的物理现象的探究，却揭示出一个光怪陆离，极度违背常识，连创始人都无法理解，却又无数次被证实是正确的理论的一众量子力学的奠基人们。人类文明的智慧闪光真的令人叹为观止，或许无尽岁月之后，有足够辨析能力的另一个文明看到或许已经灭绝的人类残存的信息，也会惊叹或惋惜于这个文明曾经的智慧吧。
>
> @卧三：
>
> 我所理解的秩，是在某个维度下所固定的条件个数，比方说，三维空间我们有x，z，y三个线性无关的条件，三个条件对应着秩为3，即秩为3时我们可以确定空间的一个点。对应此时我们只有唯一解，即为R(A) = n ，假如三维空间中，我们秩为2 即为R(A) < n ,我们们只有两个量假设x，y是固定的或者说已知的，此时我们将会有一个平面的点z不确定的点。所以我们将有一个自由变量，即为z方向上的自由变量，我们用一个向量表示这些有的点，则为我们的通解。所以维度 = 秩（确定的量）+ 基础解系个数(不确定的量)

目录：

[TOC]




## 0.线性代数的“线性”

- “线性”的严格定义如下若一个变换工满足两条性质：**齐次性和可加性**。



1. 可加性:
   $$
   L(\vec{v} +\vec{w}) = L(\vec{v})+L(\vec{w})
   $$

2. 齐次性（成比例）
   $$
   L(c\vec{v}) = cL(\vec{v})
   $$

则称L是线性的。

> 从几何角度看，
>
> 将`i向量`固定，移动 `j向量`，将形成一条直线；
>
> 将`j向量`固定，移动 `i向量`，将形成另一条直线；
>
> 每次移动向量，都是对应基向量的倍数（成比例），而基向量之间可以相加形成新的向量。
>
> 可加、可乘（成比例）
>
> 如果固定其中一个标量，让另一个标量自由变化，所产生的向量的终点会描出一条直线。
>
> **基向量线性组合的向量的集合，被称为给定向量张成的空间(span)**

- 线性的词源:出于线性方程组。线性方程组在二维情况下，有形如ax+by=c的子方程

  所谓线性关系，简单地讲就是比例关系，即两个变量按一定的比例增加或减少。

  **这种关系若用图形来表示，就是一条直线，故称线性关系。**这种关系若用方程来表示，就称为线性方程

- 线性代数 这门学科之所以包含“线性”二字，是因为它主要研究的是**向量空间（或称为线性空间）、线性变换以及线性方程组等概念。**这些概念都涉及到线性关系：



## 1.**线性相关**

可以理解为线性相关意味着停留在这个维度不增维。当一组向量成线性相关时，其中至少有一个向量能由其它向量线性表示

## **2.基向量**

空间的一组基的严格定义是这样的:张成该空间的一个线性无关向量的集合。

（向量空间的一组基是张成该空间的一个线性无关向量集）

## 3.**线性变换** 

严格意义上说，线性变换是将向量作为输入和输出的一类函数。

线性变换是操纵空间的一种手段。

它具备两个性质：

**①保持网格线平行且等距分布，②保持原点不动。**

这两点性质保证了：只要记录下i帽和j帽变换后的位置，你就能计算出一个坐标为(x,y)的向量变换后的坐标。

令人高兴的是，这种变换只需要几个数字就能描述清楚，这些数字就是变换后基向量的坐标。**线性变换由它对空间的基向量的作用完全决定。**

> 习惯上，我们将变换后i帽和j帽的坐标作为一个矩阵的列，并且将两列分别与x和y相乘后加和的结果定义为矩阵向量乘积

这样，矩阵代表一个特定的线性变换。而矩阵与向量相乘，就是将线性变换作用于那个向量。

因此**可以将线性变换看作对空间的挤压伸展。**这是因为其他任意向量都能表示为基向量的线性组合。

---

线性相关时，如果变换后的i帽和变换后的j帽是线性相关的，意味着其中一个向量是另一个的倍数，那么这个线性变换将整个二维空间挤压到它们所在一条直线上。也就是这两个线性相关向量所张成的一维空间。

以这些坐标为列所构成的矩阵为我们提供了一种描述线性变换的语言，而矩阵向量乘法就是计算线性变换作用于给定向量的一种途径。

这里重要的是。**每当你看到一个矩阵时，你都可以把它解读为对空间的一种特定变换。**

矩阵相乘：两个变化相互作用，即复合变化。这里可以联想复合函数。



**这样也可以解释矩阵的两个性质：**

1. 不具备交换律

   AB ≠ BA

   也就是为什么矩阵乘法有严格顺序要求。例如 先旋转、再剪切 和 先剪切、再旋转是不同的。

2. 具备结合律

   A(BC) 和 (AB)C 三个线性变化的相对顺序不变，

>线性代数中的“剪切”通常指的是剪切矩阵（shear matrix）或剪切变换（shear transformation），这是一种线性变换，它将一个图形沿着某一轴或平面进行倾斜，而不改变图形的大小或形状

这是通过矩阵在几何上的线性变换，证明矩阵乘法具有结合性的一个实实在在的证明。我真的鼓励你在这种想法上多做尝试，想象两个不同的变换，思考他们依次作用后会发生什么，最后用数值方法计算出矩阵乘积。

4.观察基向量的运算和线性变换，**“缩放再相加”**的过程在变换前后均适用。

对于一个矩阵，用一个元素均为x、y、z这种未知数的向量相乘，得到的新矩阵就是新的基向量。

**线性变换矩阵 \* 输入向量 = 输出向量。**

（线性变换和函数的功能是类似的，输入-处理-输出。但变换一词更强调变换本身的过程，也就是几何而非数值上的变化。）



## 4.行列式

不同矩阵代表的线性变换中，有的将空间向外拉伸，有的将空间向内挤压。理解这些线性变换的关键一点，是测量变换对空间拉伸或挤压的程度。也就是**测量一个给定区域面积增大或减小的比例**。

这个特殊的缩放比例，即**线性变换改变面积的比例 , 被称为这个变换的行列式**



**注意：**

1. 无论一个方格如何变化，对其他大小的方格来说，都会有相同变化。

   > 这是由“网格线保持平行且等距分布”这一事实推断得出的

2. 对于不是方格的形状（存在曲线的），可以用很多小方格近似。

   > 对所有小方格都进行等比例缩放，缩放的越小，精度越高。

3. 如果一个二维线性变换的行列式为0，说明它将整个平面压缩到一条线，甚至是一个点上。

   > 表现在几何上就是面积为0，没有面积。 	

   这意味着一个矩阵的行列式如果为0，这个矩阵所代表的变换就能将空间压缩到更小的维度上（降维）。



**二维空间的行列式负值**：

**定向**

> 行列式有负值。缩放一个负数是什么意思?
>
> 这会涉及到一个叫“定向”的概念。

1. 如里你将二维空间想象为一张纸，这个变换像是将纸翻转到了另一面。

2. 根据`i帽`和`j帽`来考虑。

   初始状态下，显然` j-hat` 在` i-hat` 左边，形成一个直角。

   **如果在变换之后，j帽处于i帽的右边，那么空间定向就发生了改变**

   > `j 帽`就是y轴基向量，`i -hat` 就是x轴基向量

我们称类似这样的变换改变了空间的定向。

**当空间定向改变的情况发生时，行列式为负。**

此时，行列式的绝对值依然表示区域面积的缩放比例。

> 比如说，我告诉你由(1,1)和(2,-1)为列的矩阵所代表的线性变换的行列式是-3.
>
> 这就是说变换后空间被翻转，并且面积放大为原来的3倍。



- **负的面积为什么和定向有关？**

> 这里强烈建议看3blue1brown原视频。
>
> 《线性代数的本质》行列式 5：05

当i帽靠近j帽时，空间也被压缩地更严重，这意味着行列式趋近于0；

当i帽与j帽完全重合时，行列式为0。

如果i帽继续沿着这个方向运动，行列式继续减小为负值将是一件很自然的事。

> 换一个角度看，二维的压缩，从三维视角看，就是一种旋转。
>
> 压缩时，向量`j `不动，向量 `i` 逐渐靠近 `j`。
>
> **想象 i 靠近 j ，越过 j 的过程：**
>
> 二维视角下，以i 和 j 为基 的坐标系接近时压缩，越过时拉伸。
>
> **三维视角下，把红向量j当做Z轴，想象绿向量在XOY平面上旋转——**
>
> **就像一个圆柱绕圆心的Z轴旋转，你只能看到 XoY 这一个侧切面的变化。**



- **三维空间，行列式意味着什么？**

依然是变换前后的缩放比例，不过这次缩放的是体积。

二维空间，我们考虑的是基向量i 和 j 组成的面积为1的正方形，并观察变换对它的影响。

三维空间，我们考虑的是基向量 i , j , k 组成的体积为1 的立体正方形。



**三维空间的行列式负值**：

有一种方法来描述三维空间的定向，那就是“右手定则——

- 右手食指指向i帽的方向；
- 伸出中指指向j帽的方向；
- 当你把大拇指竖起来时，它就正好指向k帽的方向。

如果线性变换后，你还可以用右手这么做，那么定向没有发生改变，行列式为正。

如果变换后，只能用左手描述了，那就是定向发生了改变，行列式为负。



**行列式的求值公式：** ad - bc 
$$
(a+b)(c+d)-ac-bd-2bc = ad-bc
$$
坐标系中， 小正方形凑成的网格面积 减去 平行四边形周围的三角形面积 = 平行四边形的面积



**行列式的一个性质证明：**
$$
det(M1M2) = det(m1) det(m2)
$$

> det行列式，M1、M2是矩阵

数值计算很麻烦。但从几何上理解：

**两个相继作用的总的线性变换对空间中几何维度造成的影响等于他们单独作用时造成影响的乘积。**

> 两次空间放缩引起的面积变化 是 两次单独放缩面积变化  倍数的乘积

在同一个变换下任何一个图形的拉伸倍率都是一样的，所以M1M2两次变换后的面积倍率结果是一致的。

> 注意：det乘det本质不是面积相乘，而是面积倍率相乘。

## 5.线性方程组

用一个式子概括：
$$
A\vec{x} = \vec{v}
$$

$$
\left(
\begin{matrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{matrix}
\right) 


\left[
\begin{matrix}
x_1 \\
x_2 \\
x_3 
\end{matrix}
\right] 

=

\left[
\begin{matrix}
a \\
b \\
c 
\end{matrix}
\right]
$$

> A 是未知数系数的矩阵，`x` 是未知数向量{x,y,z或x1,x2,x3......}， `v`是常数向量，如果是齐次的话，v 就是 很多 0 的 向量。



这个式子阐明了线性方程组问题中优美的几何直观部分：

**矩阵A代表一种线性变换，所以 求解Ax=v意味着我们去寻找一个向量x，使得它在变换后与v重合。**

> 这个时候逆矩阵的几何意义也很清晰了！

**当你逆向进行变换时，它实际上对应了另一个线性变换 —— 用`v`经过线性变换（矩阵）去寻找`x`。**

> 也就是等式两边同乘A逆，v逆向进行变换并跟踪x的动向。

这个线性变换就叫作A^-1^  A的逆矩阵！ 


> 比如说，如果A是逆时针旋转90度的变换,那么A的逆就是顺时针旋转90度的变换。
>
> 总的来说，A逆是满足以下性质的唯一变换：
>
> **首先应用A代表的变换，再应用A逆代表的变换，你会回到原始状态。**
>
> A逆乘以A等于一个“什么都不做”的矩阵，这个“什么都不做”的变换被称为“恒等变换”

**x和v重合有两种情况：**

1. 降维，两者变成一条线或一个点从而重合。
2. 不降维，在原有维度（二维、三维）上通过线性变换重合。

不降维，即存在唯一解的情况，这时存在A逆。

> A^-1^ 使得应用A变换再应用A逆变换之后，结果与恒等变换无异。

**降维即行列式为 0的情况；**

与这个方程组相关的变换将空间压缩到更低的维度上，此时没有逆变换，你不能将一条线“解压缩”为一个平面，也不能将一个点“回溯”成一条线（至少这不是一个函数能做的），你不能进行升维。

> 降维会损失信息。
>
> 空间坍缩为更低的维度后，变换信息有所丢失，无法得到其逆变换，也就是逆矩阵不存在

这样就会要求将一个单独的向量变换为一整条线的向量，但是函数只能将一个输入变换为一个输出。



**关于列空间（下面理解方程组求解要用）：**

**所有可能的变换结果(输出向量)的集合 被称为矩阵的“列空间。**

> 不管是一条直线、一个平面还是三维空间等。

矩阵的列告诉你基向量变换后的位置，这些**变换后的基向量张成的空间就是所有可能的变换结果 **;

换句话说**，列空间就是矩阵的列所张成的空间。**

> 列张成的空间 span of columns↔️列空间 Column space

**0向量一定包含在列空间中。**

> 因为线性变换要求原点位置不变。



结合这里的几何特性你可以理解：

对于线性方程组的求解情况——

- ==非齐次==

1. **增广矩阵r(A,v) 的秩和矩阵的秩r(A) 是否相等：**

   - `r(A,v) == r(A)`说明 **v 和A在一个维度上** ，至少有一个解

   - `r(A,v)  > r(A)` :说明 **v 比 A 的维度要高** ，无解

     > 列空间：列向量张成的空间`span`，忘记的看第 0 条 。
     >
     > 列空间是所有可能的 A 的列向量的线性组合构成的集合。
     >
     > 假设矩阵 A 是一个 m×n 的矩阵，那么它有 n 个列向量。这些列向量可以看作是 R^m^ (m维空间）中的向量。
     >
     > 如果这些列向量**线性无关**，它们可以张成一个 n 维的子空间，这个子空间就是 A 的列空间。
     >
     > 现在，当我们说向量 v 不在矩阵 A 的列空间中，这意味着没有一种方式通过 A 的列向量的线性组合来得到向量 v。
     >
     > **换句话说，v 向量的维度高于 A，于是不存在一个向量 x 使得 Ax = v 成立。**
     >
     > 如果我们将 A 的列向量看作是定义了一个平面或者高维空间中的一个超平面，那么 b 就是位于这个平面或超平面之外的点。

     r(A,v)  > r(A) 时，意味着 v 的维度比 A 的维度要高，A经过初等行变换会存在 全 0 行。

     >A如果线性相关，则必然存在 全 0 行；
     >
     >A如果线性无关，但比我们要追踪的 v 向量 少一行，也意味着A、v两者比较时，A下面要补充一个全0行。
     >
     >**总之就是少一个维度。**

      r(A) 里存在线性相关的列向量。于是 A 的线性变换相比 v向量 少了一个维度，找不到 x向量 经过低维线性变换 追踪到 v。

   

2. **秩的数量(A)是否和未知数n (x向量)一样**：

   - 如果一样，说明 **A 和  x 在同一维度**，x可以通过有限线性变化A  与 v重叠，**这时是唯一解。**

   - 如果不一样，说明 **A 比 x 维度低** ，x 想与 v 重叠，只能通过“降维”，**这时是无穷多解。**

     > 可以想象，物体投影到平面上只有一个影子；而从投影去猜测物体，物体可以有无限种形状。
     >
     > 同样地，经过降维压缩变换后，能够和低维的 v 重叠的 高维 x 有无穷多种。
     >
     > 例如三维→二维， 立体被压缩成一条线；
     >
     > 例如二维→一维，一条线被压缩成一个点。
     >
     > **都是无穷多解的情况。**



> 提问：增广矩阵的秩为n时，什么时候有解？
>
> 答：增广矩阵在n维度， A在n-1维度时无解。 只有当增广和A都在n维度时 才有解



- ==齐次==

  一定有解。

  > n 是 未知数个数。

  - r(A) = n ， **A 和 x 在同一维度** ，因为 v 向量是0，x 作为 0 向量，经过任何线性变换都和 v 重叠。

    > 是的，只有一个 唯一解，那就是0向量。

  - r(A) < n ,    **A 比 x 维度低** ，x 想与 0向量 重叠，只能通过线性变换来 “降维”，**这时是无穷多解。**

    > 这无穷多个解不是整个二维平面，而是二维平面/三维空间上的一条线，叫做零空间。





**于是不难理解行列式为 0 时的意义：**

行列式为0 时，意味着变换前后的面积倍率为0（压缩后面积为0），一定有维度变化。

对线性方程组，表现在齐次上就是无穷多解（线性变换降维），表现在非齐次上要么无解（v 比 A 维度高），要么也无穷多解(线性变换降维)。



## 6.秩 (Rank)

“秩”代表着变换后空间的维度
So the word **"rank"** means **the number of dimensions in the output of a transformation.**

> 列向量所张成空间的维度（基向量个数）

举例：

比如说对于2x2的矩阵，它的秩最大为2,意味着基向量仍旧能张成整个二维空间，并且矩阵的行列式不为零

但是对于3x3的矩阵，秩为2意味着空间被压缩了，但是和秩为1的情况相比，压缩并不是那么严重。

> 如果一个三维变换的行列式不为零，变换结果仍旧充满整个三维空间，那么它的秩仍为3。



**所以更精确的==秩的定义是列空间的维数==。**

当达到最大值时，意味着秩与列数相等，我们称之为“满秩”



**对一个满秩变换来说，唯一能在变换后落在原点的就是零向量自身。**

但是对一个非满秩的矩阵来说，它将空间压缩到一个更低的维度上。也就是说会有一系列向量在变换后成为零向量。

>举个例子，如果一个二维线性变换 将 非满秩（秩为1，两个基向量线性相关）平面压缩到一条直线上，那么沿某个不同方向直线上的所有向量就被压缩到原点。
>
>就好像柱子的侧面（直线）经过线性变换（旋转）变成了一个点（底面）。
>
>如果一个三维线性变换将空间压缩到一个二维平面上，同样也会有一整条线上的向量在变换后落在原点。
>
>压缩成二维直线也一样。

**零空间：**

变换后落在原点的向量的集合，被称为矩阵的“零空间”或“核。变换后一些向量落在零向量上，而“零空间”正是这些向量所构成的空间

> 比如上面例子中那一整条直线就是零空间。

**列空间的概念让我们清楚什么时候存在解，零空间的概念有助于我们理解所有可能的解的集合是什么样的。**

## 7. 非方阵矩阵

**举例分析**

- 3x2矩阵：是一个面，但仍然满秩。几何意义是将一个二维空间映射到三维空间上。

  > 二维空间输入 ，三维空间输出。

  矩阵有两列表明输入空间有两个基向量，有三行表明每一个基向量在变换后都用三个独立的坐标来描述。

- 2x3矩阵：

  > 三维空间输入 ，二维空间输出。

  几个基向量说明空间是几维；有两行表明这三个基向量在变换后都仅用两个坐标来描述，所以他们一定落在二维空间中。

**解线性方程**

- 3×2矩阵解线性方程：3个方程式，2个未知数，将二维空间投射到三维空间，要么无解，要么一个解
- 2×3矩阵解线性方程：2个方程式，3个未知数，将三维空间压缩成二维空间，要么无解，要么无穷多解

## 8.点乘和线性变换（相当劲爆）

**点乘标准形式：**
$$
\left[
\begin{matrix}
a \\ b
\end{matrix}
\right]
\cdot
\left[
\begin{matrix}
c \\ d 
\end{matrix}
\right]=

(a\cdot c)
+
(b\cdot d)
$$
**点乘几何形式：**

二维坐标系里进行演算可知，点乘就是其中一个向量在另一个向量上的投影长度，乘以被**投影**向量长度的成绩。

---

这一章的思想跨度较大（可以说很惊人、很震撼且富有美感），可以配合3b1b的图形化视频食用。

> 这里总结一下就是，用一个向量点乘另一个向量，实质上是做了一个线性变换。
>
> 点积可以看作是矩阵向量乘法的一个特例，是投影到一维的线性变换。
>
> 所以矩阵向量乘法的本质是线性变换。
>
> 线性变换的“物质载体“ ，则是其中一个向量扮演的。

**考虑一个问题**

==为什么点乘的运算过程：**对应坐标相乘并将结果相加，和投影有所联系？**==

让我们试着证明一下：

还是老样子，

1. 前提：**等距分布的点保持等距分布，从而为我们保证线性变换的基础。**

   > 否则线性变换，或者说高维向量在一维空间（数轴）上的投影就不是线性的，而是离散的。

2. 视角：这些线性变换完全由它对i帽和j帽的变换决定，所以只关注i帽、j帽。

   > 由 i帽 和 j帽 的变换，可以推出二维向量的变换。这也是以前对线性变换的分析方法，不拘泥于过程，由基向量推其他向量。
   >
   > **（u的坐标体现的是i,j向量的变化，而i,j向量就可以反映其余的改变）**



为什么要找i-hat和j-hat投影在斜数轴上？因为一开始那个把二维变一维的函数作的变换就是投影，而投影后的i-hat和j-hat的位置表示这种投影变换。

> i-hat 和 j-hat 的变换则代表了对应的所有列空间向量的变换。

定义一个从二维向量到数的线性变换，找到描述这个变换的1x2矩阵

> Projection matrix 投影矩阵 Transformation matrix 变换矩阵。

所以描述投影变换的1x2矩阵的两列，也就是 **i-hat和j-hat在斜数轴上的投影的值，根据对称性，正好分别是斜数轴u-hat的点的横纵坐标 u~x~ , u~y~ 。**

因此**空间中任意向量经过投影变换的结果，也就是投影矩阵与这个向量相乘**。

> 也就是投影矩阵[u~x~ , u~y~]与这个向量[i-hat, j-hat]相乘

这意味着，当一个二维向量v和另一个1x2的矩阵A 相乘时，**这和基向量与u帽的点积在计算上完全相同。** 

> 选择u帽长度为1并且落在一维数轴上，主要是为了体现后续u帽和i帽、j帽相互映射的对偶性



这就是为什么与单位向量的点积可以解读为将向量 v 投影到投影矩阵代表的单位向量 u 所在的直线上所得到的投影长度。

> 点积是一种将二维向量投影（降维）成一维向量（数轴）的线性变换。
>
> （高维向量在一维的投影）



矩阵向量相乘 Matrix-vector product <==> 点积 Dot product
$$
\left[
\begin{matrix}
u_x & u_y 
\end{matrix}
\right]

\left[
\begin{matrix}
x \\
y \\
\end{matrix}
\right]=
u_x \cdot x
+u_y \cdot y
$$
↑等价↓
$$
\left[
\begin{matrix}
u_x \\
u_y 
\end{matrix}
\right]

\left[
\begin{matrix}
x \\
y \\
\end{matrix}
\right]=
u_x \cdot x
+u_y \cdot y
$$


你在任何时候看到一个线性变换，它的输出空间是一维数轴。无论它是如何定义的，空间中会存在唯一的向量v与之相关，就这一意义而言，应用线性变换和与向量v做点积是一样的。

>它是数学中“对偶性”的一个实例。
>
>- 你可以说一个向量的对偶是由它定义的线性变换；
>
>- 一个多维空间到一维空间的线性变换的对偶是多维空间中的某个特定向量



**两个向量点乘，就是将其中一个向量转化为线性变换**

**有时你就会意识到，不把向量看作空间中的箭头，而把它看作线性变换的物质载体，会更容易理解向量**

向量就仿佛是一个特定变换的概念性记号。因为对我们来说，想象空间中的向量比想象整个空间移动到数轴上更加容易。



---

**举个例子：**
$$
\vec{x}\times\vec{v}=

\left[
\begin{matrix}
1 \\ -2 \\
\end{matrix}
\right]

\left[
\begin{matrix}
4 \\ 3 \\
\end{matrix}
\right]=

\left[
\begin{matrix}
1 & -2 
\end{matrix}
\right]

\left[
\begin{matrix}
4 \\ 3 \\
\end{matrix}
\right]=

4\times1
+
3\times(-2)
$$
将列向量 `[1 -2]` “横过来“ 看作是线性变换矩阵（Transformation matrix），此时，向量 v 的基向量 i-hat 和 j-hat 由默认的`[1,0]` ,`[0 1]` 经过线性变换 (乘变换矩阵) 变成了 `[1 0],[0 -2]` 。

> 这个线性变换的效果，实际上是“降维”，将两个二维基向量压缩成一条线（数轴）了，用点来表示，一个指向`+1` , 一个指向`-2`。

`[4,3]`这个向量跟随基向量一起线性变换（降维），变成了指向`-2`的向量。

> `4*(+1 0)` 表示 i-hat 长度变成4倍， `3 *(0 -2)`表示 j-hat 长度变 3 倍，叠加在一起就是 - 2 。

- 当你完全从数值角度进行计算时，它就是矩阵向量乘法，而点积可以看作是矩阵向量乘法的一个特例。
- 但从几何角度出发，向量`[1  -2]`转化为线性变换的载体，使另一个向量`[4 3]`的基向量变换了，于是 向量`[4 3]`的基向量线性变换后进行放缩（乘以对应倍率），得到的新向量，数值结果和向量乘法一样。



## 9.叉乘

**叉乘标准形式：**
$$
\left[
\begin{matrix}
a \\ b \\
\end{matrix}
\right]
\times
\left[
\begin{matrix}
c \\ d \\
\end{matrix}
\right]=

(a\times d)-(b\times c)

\
$$

$$
\vec{a}\times\vec{b}=

\left[
\begin{matrix}
a1 \\ a2 \\ a3
\end{matrix}
\right]
\times
\left[
\begin{matrix}
b1 \\ b2 \\ b3
\end{matrix}
\right]=

\left|
\begin{matrix}
\widehat{i} & \widehat{j} & \widehat{k}
\\ a1 & a2 & a3
\\ b1 & b2 & b3
\end{matrix}
\right|
$$

> i 的余子式 + j 的余子式 + k 的余子式

**叉乘几何意义：**

a~1~b~1~两个向量分别平移相交成 a~2~b~2~，四个向量a~1~a~2~b~1~b~2~围成的平行四边形的面积就是叉乘的绝对值。

正负性（定向问题）：以`a * b`为例。如果在二维平面上，a 在 b 右边（b需要经过右旋转到a），那么a*b就是正值。

如果a 在 b 左边（b需要经过左旋转到a），那么a*b就是负值。

> 注意：这是说顺序对叉乘有影响。

**几何角度观察定向：** i-hat 在 j-hat 右边，`i * j = +1` 。那么 a 在 b 右边，`a * b = +`

> 基向量的顺序就是定向的基础



---



**行列式的面积和叉乘有很大关系。**
$$
\left[
\begin{matrix}
a \\ b \\
\end{matrix}
\right]
\times
\left[
\begin{matrix}
c \\ d \\
\end{matrix}
\right]=

\left|
\begin{matrix}
a & c\\ b & d
\end{matrix}
\right|=

(a\times d)-(b\times c)

\
$$


`a,b`两个向量相乘 ，你只需要将 a向量作为行列式第一列，b向量作为行列式第二列，然后直接计算行列式。

所以：

1. 这里可以一个解释行列式性质：交换任意两行，行列式正负性改变。

   **几何上，两个向量叉乘交换顺序了，定向改变了，向量的基改变了，行列式为负不是很自然的吗？**

2. 你可能注意到一点，**当两个向量接近垂直时，他们的叉积最大。**因为此时构成的平行四边形面积最大。
3. **放大其中一个向量3倍，那么叉乘也增大3倍**。从面积上理解也很自然。

---

根据上面的二维向量叉乘，如果让我们猜测三维向量的叉乘，大概是：
$$
\left[
\begin{matrix}
u_1 \\ v_1 \\ w_1
\end{matrix}
\right]

\times

\left[
\begin{matrix}
u_2\\ v_2 \\ w_2
\end{matrix}
\right]

\times

\left[
\begin{matrix}
u_3 \\v_3 \\ w_3
\end{matrix}
\right]=

\left|
\begin{matrix}
u_1 & u_2  & u_3\\ v_1 & v_2 & v_2\\w_1 & w_2 & w_3
\end{matrix}
\right|
(这里是错误的猜想)
$$
输入三个向量，输出一个行列式（数值），这个行列式的数值是三维空间里，三个向量组成的体积。

> 虽然这么想很自然，却又**还不是真正的叉积**，但已经很接近了。

**真正的叉积是通过两个三维向量生成一个新的三维向量。**

叉积的结果不是一个数，而是一个向量。很奇怪吧？

这个**向量的长度就是平行四边形的面积**，而这个**向量的方向与平行四边形(所在的面)垂直。**

垂直方向有两个，至于是哪一个，要靠“**右手定则”：**

- 食指指向i帽的方向；
- 伸出中指指向j帽的方向；
- 当你把大拇指竖起来时，它就正好指向k帽的方向。

> 注意：第4节行列式也出现了右手定则。

==这里要证明一件事：**为什么叉积生成的新三维向量有这个性质？**==

**思路一：直接数值运算**

> 很直接很暴力，也很麻烦。这不是我们今天要讨论的。

**思路二：几何思路**

> 前提：**对偶性思想**——上一节点乘用到的思想。
>
> 每当你看到一个(多维)空间到数轴的线性变换时，它都与空间中的唯一个向量对应，也就是说应用线性变换和与这个向量点乘等价，数值上说，这是因为这类线性变换可以用一个只有一行的矩阵描述，而它的每一列给出了变换后基向量的位置。
>
> 这里的收获在于，每当你看到一个从空间到数轴的线性变换，你都能够找到一个向量，被称为**这个变换的对偶向量**（dual vector），使得应用线性变换和与对偶向量点乘等价。

叉积的运算同样体现了对偶性思想。

现在，我们将刚刚上面的猜想叉积的第一个向量u看作可变向量，比如(x,y,z)，而v和w保持不变，那么我们就有一个**从三维空间到数轴的函数**了。

> 这个函数根据v和w来定义（因变量），输入`(x,y,z)`（自变量）产生行列式（结果）。
>
> 你输入一个向量(x,y,z)，然后通过矩阵的行列式得到一个数，这个向量的第一列是(x, y,z)，其余两列是常向量v和w的坐标。

$$
f\left(
\left[
\begin{matrix}
x \\ y \\ z
\end{matrix}
\right]
\right)=
det
\left(
\left[
\begin{matrix}
x & v_1 & w_1 \\
y & v_2 & w_2 \\
z & v_3 & w_3
\end{matrix}
\right]
\right)
$$

$$
变量variable=
\vec{u}=
\left[
\begin{matrix}
x \\ y \\ z
\end{matrix}
\right]

& \vec{v} = 
\left[
\begin{matrix}
v_1 \\ v_2 \\ v_3 
\end{matrix}
\right]

& \vec{w} = 
\left[
\begin{matrix}
w_1 \\ w_2 \\ w_3
\end{matrix}
\right]
$$

> 这里将向量写作矩阵的列，而**教科书中大多将向量写作矩阵的行**。两种结果没有差异，因为转置不改变行列式的值。这里选择按列处理向量是为了更加直观。

这个函数的几何意义是，对于任一输入的向量(x, y,z), 考虑由它和v与w确定的**平行六面体,得到它的体积，然后根据定向确定符号。**

> **这个函数是线性的**，由行列式性质可知：
>
> 行列式就是三个向量所夹的平行六面体的体积。这个平行六面体的底面积是定下来的，所以如果高是线性的体积就是线性的。

一旦知道它是线性的，你就知道**可以通过矩阵乘法来描述**这个函数。

即**这个函数从三维空间到一维空间的每个线性变换，都存在一个1x3矩阵来代表这个变换。**	

> 对偶性告诉我们，你可以将这个1*3矩阵立起来，并且将整个变换看作与这个特定向量的点积。

$$
(神秘1\times3矩阵)
\left[
\begin{matrix}
? \\ ? \\ ?
\end{matrix}
\right]
\left[
\begin{matrix}
x \\ y \\ z
\end{matrix}
\right]=

(这是\vec{p})
\left[
\begin{matrix}
p_1 \\ p_2 \\ p_3
\end{matrix}
\right]
\left[
\begin{matrix}
x \\ y \\ z
\end{matrix}
\right]=


det
\left(
\left[
\begin{matrix}
x & v_1 & w_1 \\
y & v_2 & w_2 \\
z & v_3 & w_3
\end{matrix}
\right]
\right)
$$

我们要找的就是这个特殊的三维向量——我称之为p向量。由上式可知，p与其他任一向量(x,y,z)的点积等于一个3x3矩阵的行列式。

**对于这个式子，有两种理解角度：计算和几何**

- 计算

  上式经过点乘：
  $$
  p1\cdot x+p2\cdot y+p3\cdot z =
  \begin{matrix}
  x(v2\cdot w3-v3\cdot w2)+ \\
  y(v3\cdot w1-v1\cdot w3)+ \\
  z(v1\cdot w2-v2\cdot w1)
  \end{matrix}
  $$
  所以很明显可以看出，我们要找的 p向量 的值就藏在右边的`v`和`w`坐标的线性组合中。
  $$
  \vec{p}=
  \left[
  \begin{matrix}
  p1 \\ p2 \\ p3
  \end{matrix}
  \right]=
  
  \begin{cases}
  p1 = v2\cdot w3-v3\cdot w2 \\
  p2=v3\cdot w1-v1\cdot w3 \\
  p3=v1\cdot w2-v2\cdot w1
  \end{cases}
  $$

  > 点积本是上是替换掉了原有的基向量，而叉积的行列式定义中有三个基向量正好可以用点积操作替换掉。

  像这样合并x、y和z前面的常数项，和把i帽、i帽和k帽放进矩阵第一列进行计算，然后合并各项前面的系数没有区别。

  > 把i帽、i帽和k帽放进矩阵第一列进行计算，正是老师根据教科书，叫我们死记硬背的叉积公式！

  **在矩阵中插入i帽、j帽和k帽不过是在传递一个信号，告诉我们应该把这些系数解读为一个向量的坐标。**

  

  好了，推导结束了。总结一下：

  **当你将向量p和某个向量`(x,y,z)`点乘时，所得结果等于一个由`(x,y,z)`和v与w确定的平行六面体的有向体积，数值上是一个3x3矩阵的行列式，这个行列式第一列为(x,y,z)，其余两列为v和w的坐标。**

  > 这是计算角度的总结。
  >
  > 让我们从几何角度看看，什么样的向量p才能满足这一特殊性质 ?

- 几何

  - 对于点积，向量p与其他向量的点积的几何解释，是将其他向量投影到p上，然后将投影长度与p的长度相乘。

  - 而对于叉积，上文中我们用一个线性函数来代表它。

    我们找到的线性函数对于给定向量的作用，也就是 p 作为一个 1x3矩阵，代表着一个线性变换；这个线性变换**将向量投影到垂直于v和w的直线上，然后将投影长度与v和w张成的平行四边形的面积相乘。**

    >这和`(x,y,z`) 与 `垂直于v和w且长度为平行四边形面积的向量`点乘是同一回事。

  首先获得由v和w确定的平行四边形的面积，乘以向量(x,y,z)在垂直于平行四边形方向上的分量(不是(x,y,z)的长度，是投影)，我们就可以得到这个平行六面体的体积，

  > 总结一下就是底面积(v,w)乘高(p投影)。

  更重要的是，如果你选择了合适的向量方向(点积为正)就会与(x,y,z)、v和w满足右手定则的情况相吻合。

  

  **这意味着我们找到了这个p，使得p与和某个向量(x,y,z)点乘时，所得结果等于和上面计算得来的3x3矩阵的行列式是一致的。**

  这就是我们要找的，计算和几何得出的同一个向量 p 。